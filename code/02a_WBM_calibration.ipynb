{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58728ec0-f35e-4ff7-aeb9-8cfa3bfead9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "import dask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from water_balance_jax import wbm_jax, construct_Kpet_vec\n",
    "from initial_params_log_vic import initial_params, constants\n",
    "from param_bounds_log_vic import params_lower, params_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dd5a8a-c642-45c7-a747-4d94d0542cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#### Directories ####\n",
    "#####################\n",
    "project_data_path = \"/storage/group/pches/default/users/dcl5300/wbm_soilM_crop_uc_lafferty-etal-2024-tbd_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537d7057-2338-48f6-ac02-db340021dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter names\n",
    "param_names_vic = [\n",
    "'awCap_sand',\n",
    "'awCap_loamy_sand',\n",
    "'awCap_sandy_loam',\n",
    "'awCap_silt_loam',\n",
    "'awCap_silt',\n",
    "'awCap_loam',\n",
    "'awCap_sandy_clay_loam',\n",
    "'awCap_silty_clay_loam',\n",
    "'awCap_clay_loam',\n",
    "'awCap_sandy_clay',\n",
    "'awCap_silty_clay',\n",
    "'awCap_clay',\n",
    "'wiltingp_sand',\n",
    "'wiltingp_loamy_sand',\n",
    "'wiltingp_sandy_loam',\n",
    "'wiltingp_silt_loam',\n",
    "'wiltingp_silt',\n",
    "'wiltingp_loam',\n",
    "'wiltingp_sandy_clay_loam',\n",
    "'wiltingp_silty_clay_loam',\n",
    "'wiltingp_clay_loam',\n",
    "'wiltingp_sandy_clay',\n",
    "'wiltingp_silty_clay',\n",
    "'wiltingp_clay',\n",
    "\"alpha_claycoef\", \"alpha_sandcoef\", \"alpha_siltcoef\", \\\n",
    "\"betaHBV_claycoef\", \"betaHBV_sandcoef\", \"betaHBV_siltcoef\", \"betaHBV_elevcoef\", \\\n",
    "\"GS_start_corn\", \"GS_end_corn\", \"L_ini_corn\", \"L_dev_corn\", \"L_mid_corn\", \"Kc_ini_corn\", \"Kc_mid_corn\", \"Kc_end_corn\", \"K_min_corn\", \"K_max_corn\", \\\n",
    "\"GS_start_cotton\", \"GS_end_cotton\", \"L_ini_cotton\", \"L_dev_cotton\", \"L_mid_cotton\", \"Kc_ini_cotton\", \"Kc_mid_cotton\", \"Kc_end_cotton\", \"K_min_cotton\", \"K_max_cotton\", \\\n",
    "\"GS_start_rice\", \"GS_end_rice\", \"L_ini_rice\", \"L_dev_rice\", \"L_mid_rice\", \"Kc_ini_rice\", \"Kc_mid_rice\", \"Kc_end_rice\", \"K_min_rice\", \"K_max_rice\", \\\n",
    "\"GS_start_sorghum\", \"GS_end_sorghum\", \"L_ini_sorghum\", \"L_dev_sorghum\", \"L_mid_sorghum\", \"Kc_ini_sorghum\", \"Kc_mid_sorghum\", \"Kc_end_sorghum\", \"K_min_sorghum\", \"K_max_sorghum\",\\\n",
    "\"GS_start_soybeans\", \"GS_end_soybeans\", \"L_ini_soybeans\", \"L_dev_soybeans\", \"L_mid_soybeans\", \"Kc_ini_soybeans\", \"Kc_mid_soybeans\", \"Kc_end_soybeans\", \"K_min_soybeans\", \"K_max_soybeans\", \\\n",
    "\"GS_start_wheat\", \"GS_end_wheat\", \"L_ini_wheat\", \"L_dev_wheat\", \"L_mid_wheat\", \"Kc_ini_wheat\", \"Kc_mid_wheat\", \"Kc_end_wheat\", \"K_min_wheat\", \"K_max_wheat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f6e822-4384-416d-8179-17c4a908141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter names\n",
    "param_names = [\n",
    "\"awCap_scalar\", \"wiltingp_scalar\", \\\n",
    "    # \"awCap_claycoef\", \"awCap_sandcoef\", \"awCap_siltcoef\", \\\n",
    "    # \"wiltingp_claycoef\", \"wiltingp_sandcoef\", \"wiltingp_siltcoef\", \\\n",
    "\"alpha_claycoef\", \"alpha_sandcoef\", \"alpha_siltcoef\", \\\n",
    "\"betaHBV_claycoef\", \"betaHBV_sandcoef\", \"betaHBV_siltcoef\", \"betaHBV_elevcoef\", \\\n",
    "\"GS_start_corn\", \"GS_end_corn\", \"L_ini_corn\", \"L_dev_corn\", \"L_mid_corn\", \"Kc_ini_corn\", \"Kc_mid_corn\", \"Kc_end_corn\", \"K_min_corn\", \"K_max_corn\", \\\n",
    "\"GS_start_cotton\", \"GS_end_cotton\", \"L_ini_cotton\", \"L_dev_cotton\", \"L_mid_cotton\", \"Kc_ini_cotton\", \"Kc_mid_cotton\", \"Kc_end_cotton\", \"K_min_cotton\", \"K_max_cotton\", \\\n",
    "\"GS_start_rice\", \"GS_end_rice\", \"L_ini_rice\", \"L_dev_rice\", \"L_mid_rice\", \"Kc_ini_rice\", \"Kc_mid_rice\", \"Kc_end_rice\", \"K_min_rice\", \"K_max_rice\", \\\n",
    "\"GS_start_sorghum\", \"GS_end_sorghum\", \"L_ini_sorghum\", \"L_dev_sorghum\", \"L_mid_sorghum\", \"Kc_ini_sorghum\", \"Kc_mid_sorghum\", \"Kc_end_sorghum\", \"K_min_sorghum\", \"K_max_sorghum\",\\\n",
    "\"GS_start_soybeans\", \"GS_end_soybeans\", \"L_ini_soybeans\", \"L_dev_soybeans\", \"L_mid_soybeans\", \"Kc_ini_soybeans\", \"Kc_mid_soybeans\", \"Kc_end_soybeans\", \"K_min_soybeans\", \"K_max_soybeans\", \\\n",
    "\"GS_start_wheat\", \"GS_end_wheat\", \"L_ini_wheat\", \"L_dev_wheat\", \"L_mid_wheat\", \"Kc_ini_wheat\", \"Kc_mid_wheat\", \"Kc_end_wheat\", \"K_min_wheat\", \"K_max_wheat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769dfcf-a99d-4528-8378-f9f7398fb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "### Dask ###\n",
    "############\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    # account=\"pches\",\n",
    "    account=\"open\",\n",
    "    cores=1,\n",
    "    memory=\"10GiB\",\n",
    "    walltime=\"00:30:00\"\n",
    ")\n",
    "cluster.scale(jobs=25)  # ask for jobs\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a624d-541f-4495-b2dc-17d804a8b3c0",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab53d502-b533-4e02-8cda-3f0431b901c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inputs(subset_name, obs_name, remove_nans):\n",
    "    ######################\n",
    "    # Read obs\n",
    "    obs = np.load(f'{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/{obs_name}_validation.npy')\n",
    "\n",
    "    ######################\n",
    "    # Read and extract inputs\n",
    "    npz = np.load(f\"{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/inputs.npz\")\n",
    "\n",
    "    # Meteo forcing\n",
    "    tas = npz['tas']\n",
    "    prcp = npz['prcp']\n",
    "\n",
    "    # LAI\n",
    "    lai = npz['lai']\n",
    "\n",
    "    # Soil properties\n",
    "    awCap = npz['awCap']\n",
    "    wiltingp = npz['wiltingp']\n",
    "    clayfrac = npz['clayfrac']\n",
    "    sandfrac = npz['sandfrac']\n",
    "    siltfrac = npz['siltfrac']\n",
    "\n",
    "    # Land use\n",
    "    corn = npz['corn']\n",
    "    cotton = npz['cotton']\n",
    "    rice = npz['rice']\n",
    "    sorghum = npz['sorghum']\n",
    "    soybeans = npz['soybeans']\n",
    "    durum_wheat = npz['durum_wheat']\n",
    "    spring_wheat = npz['spring_wheat']\n",
    "    winter_wheat = npz['winter_wheat']\n",
    "    wheat = durum_wheat + spring_wheat + winter_wheat\n",
    "    \n",
    "    cropland_other = npz['cropland_other']\n",
    "    water = npz['water']\n",
    "    evergreen_needleleaf = npz['evergreen_needleleaf']\n",
    "    evergreen_broadleaf = npz['evergreen_broadleaf']\n",
    "    deciduous_needleleaf = npz['deciduous_needleleaf']\n",
    "    deciduous_broadleaf = npz['deciduous_broadleaf']\n",
    "    mixed_forest = npz['mixed_forest']\n",
    "    woodland = npz['woodland']\n",
    "    wooded_grassland = npz['wooded_grassland']\n",
    "    closed_shurbland = npz['closed_shurbland']\n",
    "    open_shrubland = npz['open_shrubland']\n",
    "    grassland = npz['grassland']\n",
    "    barren = npz['barren']\n",
    "    urban = npz['urban']\n",
    "    \n",
    "    all_other = cropland_other + water + evergreen_needleleaf + evergreen_broadleaf + deciduous_needleleaf + deciduous_broadleaf + mixed_forest + woodland + wooded_grassland + closed_shurbland + open_shrubland + grassland + barren + urban\n",
    "    \n",
    "    # Geophysical\n",
    "    elev_std = npz['elev_std']\n",
    "    \n",
    "    lats = npz['lats']\n",
    "    lons = npz['lons']\n",
    "    \n",
    "    # Initial conditions\n",
    "    Ws_init = npz['soilMoist_init']\n",
    "\n",
    "    ##########################\n",
    "    # Prepare inputs for vmap:\n",
    "    # spatial dimensions need to be collapsed and first\n",
    "    # NaN gridpoints need to be removed\n",
    "    nx = tas.shape[0]\n",
    "    ny = tas.shape[1]\n",
    "    nt = tas.shape[2]\n",
    "\n",
    "    assert nt % 365 == 0\n",
    "    nyrs = int(nt / 365)\n",
    "\n",
    "    ## Obs\n",
    "    ys = obs.reshape(nx * ny, nt)\n",
    "    nan_inds_obs = jnp.isnan(ys).any(axis=1)\n",
    "\n",
    "    ## Forcing: all days\n",
    "    tas_in = tas.reshape(nx * ny, nt)\n",
    "    prcp_in = prcp.reshape(nx * ny, nt)\n",
    "\n",
    "    x_forcing_nt = jnp.stack([tas_in, prcp_in], axis=1)\n",
    "    nan_inds_forcing_nt = jnp.isnan(x_forcing_nt).any(axis=(1,2))\n",
    "\n",
    "    ## Forcing: yearly\n",
    "    lai_in = lai.reshape(nx * ny, 365)\n",
    "    x_forcing_nyrs = lai_in\n",
    "    nan_inds_forcing_nyrs = jnp.isnan(x_forcing_nyrs).any(axis=1)\n",
    "\n",
    "    ## Maps\n",
    "    awCap_in = awCap.reshape(nx * ny)\n",
    "    wiltingp_in = wiltingp.reshape(nx * ny)\n",
    "\n",
    "    Ws_init_in = Ws_init.reshape(nx * ny)\n",
    "\n",
    "    clayfrac_in = clayfrac.reshape(nx * ny)\n",
    "    sandfrac_in = sandfrac.reshape(nx * ny)\n",
    "    siltfrac_in = siltfrac.reshape(nx * ny)\n",
    "\n",
    "    lats_in = np.tile(lats, nx)\n",
    "    elev_std_in = elev_std.reshape(nx * ny)\n",
    "\n",
    "    corn_in = corn.reshape(nx * ny)\n",
    "    cotton_in = cotton.reshape(nx * ny)\n",
    "    rice_in = rice.reshape(nx * ny)\n",
    "    sorghum_in = sorghum.reshape(nx * ny)\n",
    "    soybeans_in = soybeans.reshape(nx * ny)\n",
    "    wheat_in = wheat.reshape(nx * ny)\n",
    "\n",
    "    all_other_in = all_other.reshape(nx * ny)\n",
    "\n",
    "    x_maps = jnp.stack([awCap_in, wiltingp_in, \n",
    "                        Ws_init_in, \n",
    "                        clayfrac_in, sandfrac_in, siltfrac_in, \n",
    "                        lats_in, elev_std_in,\n",
    "                        corn_in, cotton_in, rice_in, sorghum_in, soybeans_in, wheat_in],\n",
    "                       axis=1)\n",
    "    nan_inds_maps = jnp.isnan(x_maps).any(axis=1)\n",
    "\n",
    "    # Remove NaNs if desired\n",
    "    if remove_nans:\n",
    "        nan_inds = nan_inds_obs + nan_inds_forcing_nt + nan_inds_forcing_nyrs + nan_inds_maps\n",
    "        ys = ys[~nan_inds]\n",
    "        x_forcing_nt = x_forcing_nt[~nan_inds]\n",
    "        x_forcing_nyrs = x_forcing_nyrs[~nan_inds]\n",
    "        x_maps = x_maps[~nan_inds]\n",
    "\n",
    "    # Return\n",
    "    return ys, x_forcing_nt, x_forcing_nyrs, x_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b888be-c8a9-496e-99a1-800df151b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inputs_vic(subset_name, obs_name, remove_nans):\n",
    "    ######################\n",
    "    # Read obs\n",
    "    obs = np.load(f'{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/{obs_name}_validation.npy')\n",
    "\n",
    "    ######################\n",
    "    # Read and extract inputs\n",
    "    npz = np.load(f\"{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/inputs_SC.npz\")\n",
    "\n",
    "    # Meteo forcing\n",
    "    tas = npz['tas']\n",
    "    prcp = npz['prcp']\n",
    "\n",
    "    # LAI\n",
    "    lai = npz['lai']\n",
    "\n",
    "    # Soil properties\n",
    "    sand = npz['sand']\n",
    "    loamy_sand = npz['loamy_sand']\n",
    "    sandy_loam = npz['sandy_loam']\n",
    "    silt_loam = npz['silt_loam']\n",
    "    silt = npz['silt']\n",
    "    loam = npz['loam']\n",
    "    sandy_clay_loam = npz['sandy_clay_loam']\n",
    "    silty_clay_loam = npz['silty_clay_loam']\n",
    "    clay_loam = npz['clay_loam']\n",
    "    sandy_clay = npz['sandy_clay']\n",
    "    silty_clay = npz['silty_clay']\n",
    "    clay = npz['clay']\n",
    "    \n",
    "    clayfrac = npz['clayfrac']\n",
    "    sandfrac = npz['sandfrac']\n",
    "    siltfrac = npz['siltfrac']\n",
    "\n",
    "    rootDepth = npz['rootDepth']\n",
    "\n",
    "    # Land use\n",
    "    corn = npz['corn']\n",
    "    cotton = npz['cotton']\n",
    "    rice = npz['rice']\n",
    "    sorghum = npz['sorghum']\n",
    "    soybeans = npz['soybeans']\n",
    "    durum_wheat = npz['durum_wheat']\n",
    "    spring_wheat = npz['spring_wheat']\n",
    "    winter_wheat = npz['winter_wheat']\n",
    "    wheat = durum_wheat + spring_wheat + winter_wheat\n",
    "    \n",
    "    cropland_other = npz['cropland_other']\n",
    "    water = npz['water']\n",
    "    evergreen_needleleaf = npz['evergreen_needleleaf']\n",
    "    evergreen_broadleaf = npz['evergreen_broadleaf']\n",
    "    deciduous_needleleaf = npz['deciduous_needleleaf']\n",
    "    deciduous_broadleaf = npz['deciduous_broadleaf']\n",
    "    mixed_forest = npz['mixed_forest']\n",
    "    woodland = npz['woodland']\n",
    "    wooded_grassland = npz['wooded_grassland']\n",
    "    closed_shurbland = npz['closed_shurbland']\n",
    "    open_shrubland = npz['open_shrubland']\n",
    "    grassland = npz['grassland']\n",
    "    barren = npz['barren']\n",
    "    urban = npz['urban']\n",
    "    \n",
    "    all_other = cropland_other + water + evergreen_needleleaf + evergreen_broadleaf + deciduous_needleleaf + deciduous_broadleaf + mixed_forest + woodland + wooded_grassland + closed_shurbland + open_shrubland + grassland + barren + urban\n",
    "    \n",
    "    # Geophysical\n",
    "    elev_std = npz['elev_std']\n",
    "    \n",
    "    lats = npz['lats']\n",
    "    lons = npz['lons']\n",
    "    \n",
    "    # Initial conditions\n",
    "    Ws_init = npz['soilMoist_init']\n",
    "\n",
    "    ##########################\n",
    "    # Prepare inputs for vmap:\n",
    "    # spatial dimensions need to be collapsed and first\n",
    "    # NaN gridpoints need to be removed\n",
    "    nx = tas.shape[0]\n",
    "    ny = tas.shape[1]\n",
    "    nt = tas.shape[2]\n",
    "\n",
    "    assert nt % 365 == 0\n",
    "    nyrs = int(nt / 365)\n",
    "\n",
    "    ## Obs\n",
    "    ys = obs.reshape(nx * ny, nt)\n",
    "    nan_inds_obs = jnp.isnan(ys).any(axis=1)\n",
    "\n",
    "    ## Forcing: all days\n",
    "    tas_in = tas.reshape(nx * ny, nt)\n",
    "    prcp_in = prcp.reshape(nx * ny, nt)\n",
    "\n",
    "    x_forcing_nt = jnp.stack([tas_in, prcp_in], axis=1)\n",
    "    nan_inds_forcing_nt = jnp.isnan(x_forcing_nt).any(axis=(1,2))\n",
    "\n",
    "    ## Forcing: yearly\n",
    "    lai_in = lai.reshape(nx * ny, 365)\n",
    "    x_forcing_nyrs = lai_in\n",
    "    nan_inds_forcing_nyrs = jnp.isnan(x_forcing_nyrs).any(axis=1)\n",
    "\n",
    "    ## Maps\n",
    "    sand_in = sand.reshape(nx * ny)\n",
    "    loamy_sand_in = loamy_sand.reshape(nx * ny)\n",
    "    sandy_loam_in = sandy_loam.reshape(nx * ny)\n",
    "    silt_loam_in = silt_loam.reshape(nx * ny)\n",
    "    silt_in = silt.reshape(nx * ny)\n",
    "    loam_in = loam.reshape(nx * ny)\n",
    "    sandy_clay_loam_in = sandy_clay_loam.reshape(nx * ny)\n",
    "    silty_clay_loam_in = silty_clay_loam.reshape(nx * ny)\n",
    "    clay_loam_in = clay_loam.reshape(nx * ny)\n",
    "    sandy_clay_in = sandy_clay.reshape(nx * ny)\n",
    "    silty_clay_in = silty_clay.reshape(nx * ny)\n",
    "    clay_in = clay.reshape(nx * ny)\n",
    "\n",
    "    Ws_init_in = Ws_init.reshape(nx * ny)\n",
    "\n",
    "    clayfrac_in = clayfrac.reshape(nx * ny)\n",
    "    sandfrac_in = sandfrac.reshape(nx * ny)\n",
    "    siltfrac_in = siltfrac.reshape(nx * ny)\n",
    "\n",
    "    rootDepth_in = rootDepth.reshape(nx * ny)\n",
    "\n",
    "    lats_in = np.tile(lats, nx)\n",
    "    elev_std_in = elev_std.reshape(nx * ny)\n",
    "\n",
    "    corn_in = corn.reshape(nx * ny)\n",
    "    cotton_in = cotton.reshape(nx * ny)\n",
    "    rice_in = rice.reshape(nx * ny)\n",
    "    sorghum_in = sorghum.reshape(nx * ny)\n",
    "    soybeans_in = soybeans.reshape(nx * ny)\n",
    "    wheat_in = wheat.reshape(nx * ny)\n",
    "\n",
    "    all_other_in = all_other.reshape(nx * ny)\n",
    "\n",
    "    x_maps = jnp.stack([sand_in,\n",
    "                        loamy_sand_in,\n",
    "                        sandy_loam_in,\n",
    "                        silt_loam_in,\n",
    "                        silt_in,\n",
    "                        loam_in,\n",
    "                        sandy_clay_loam_in,\n",
    "                        silty_clay_loam_in,\n",
    "                        clay_loam_in,\n",
    "                        sandy_clay_in,\n",
    "                        silty_clay_in,\n",
    "                        clay_in,\n",
    "                        Ws_init_in, \n",
    "                        clayfrac_in, sandfrac_in, siltfrac_in, \n",
    "                        rootDepth_in,\n",
    "                        lats_in, elev_std_in,\n",
    "                        corn_in, cotton_in, rice_in, sorghum_in, soybeans_in, wheat_in],\n",
    "                       axis=1)\n",
    "    nan_inds_maps = jnp.isnan(x_maps).any(axis=1)\n",
    "\n",
    "    # Remove NaNs if desired\n",
    "    if remove_nans:\n",
    "        nan_inds = nan_inds_obs + nan_inds_forcing_nt + nan_inds_forcing_nyrs + nan_inds_maps\n",
    "        ys = ys[~nan_inds]\n",
    "        x_forcing_nt = x_forcing_nt[~nan_inds]\n",
    "        x_forcing_nyrs = x_forcing_nyrs[~nan_inds]\n",
    "        x_maps = x_maps[~nan_inds]\n",
    "\n",
    "    # Return\n",
    "    return ys, x_forcing_nt, x_forcing_nyrs, x_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6284c44-21a1-42ef-abeb-2119e31bbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_vic(theta, constants, x_forcing_nt, x_forcing_nyrs, x_maps):\n",
    "    # Read inputs\n",
    "    tas, prcp = x_forcing_nt\n",
    "    lai = x_forcing_nyrs\n",
    "    \n",
    "    sand, loamy_sand, sandy_loam, silt_loam, silt, loam, sandy_clay_loam, silty_clay_loam, clay_loam, sandy_clay, silty_clay, clay, \\\n",
    "    Ws_init, \\\n",
    "    clayfrac, sandfrac, siltfrac, \\\n",
    "    rootDepth, \\\n",
    "    lats, elev_std, \\\n",
    "    corn, cotton, rice, sorghum, soybeans, wheat \\\n",
    "    = x_maps\n",
    "\n",
    "    # Define all constants\n",
    "    Ts, Tm, Wi_init, Sp_init = constants \n",
    "    \n",
    "    # Define all params\n",
    "    awCap_sand, awCap_loamy_sand, awCap_sandy_loam, awCap_silt_loam, awCap_silt, awCap_loam, awCap_sandy_clay_loam, awCap_silty_clay_loam, awCap_clay_loam, awCap_sandy_clay, awCap_silty_clay, awCap_clay, \\\n",
    "    wiltingp_sand, wiltingp_loamy_sand, wiltingp_sandy_loam, wiltingp_silt_loam, wiltingp_silt, wiltingp_loam, wiltingp_sandy_clay_loam, wiltingp_silty_clay_loam, wiltingp_clay_loam, wiltingp_sandy_clay, wiltingp_silty_clay, wiltingp_clay, \\\n",
    "    alpha_claycoef, alpha_sandcoef, alpha_siltcoef, \\\n",
    "    betaHBV_claycoef, betaHBV_sandcoef, betaHBV_siltcoef, betaHBV_elevcoef, \\\n",
    "    GS_start_corn, GS_end_corn, L_ini_corn, L_dev_corn, L_mid_corn, Kc_ini_corn, Kc_mid_corn, Kc_end_corn, K_min_corn, K_max_corn, \\\n",
    "    GS_start_cotton, GS_end_cotton, L_ini_cotton, L_dev_cotton, L_mid_cotton, Kc_ini_cotton, Kc_mid_cotton, Kc_end_cotton, K_min_cotton, K_max_cotton, \\\n",
    "    GS_start_rice, GS_end_rice, L_ini_rice, L_dev_rice, L_mid_rice, Kc_ini_rice, Kc_mid_rice, Kc_end_rice, K_min_rice, K_max_rice,  \\\n",
    "    GS_start_sorghum, GS_end_sorghum, L_ini_sorghum, L_dev_sorghum, L_mid_sorghum, Kc_ini_sorghum, Kc_mid_sorghum, Kc_end_sorghum, K_min_sorghum, K_max_sorghum, \\\n",
    "    GS_start_soybeans, GS_end_soybeans, L_ini_soybeans, L_dev_soybeans, L_mid_soybeans, Kc_ini_soybeans, Kc_mid_soybeans, Kc_end_soybeans, K_min_soybeans, K_max_soybeans, \\\n",
    "    GS_start_wheat, GS_end_wheat, L_ini_wheat, L_dev_wheat, L_mid_wheat, Kc_ini_wheat, Kc_mid_wheat, Kc_end_wheat, K_min_wheat, K_max_wheat \\\n",
    "    = jnp.exp(theta)\n",
    "    # = theta\n",
    "    # = jnp.exp(theta)\n",
    "\n",
    "    # Construct Kpet as weighted average\n",
    "    Kpet_corn = construct_Kpet_vec(GS_start_corn, GS_end_corn, L_ini_corn, L_dev_corn, L_mid_corn, 1. - (L_ini_corn + L_dev_corn + L_mid_corn), Kc_ini_corn, Kc_mid_corn, Kc_end_corn, K_min_corn, K_max_corn, lai)\n",
    "    Kpet_cotton = construct_Kpet_vec(GS_start_cotton, GS_end_cotton, L_ini_cotton, L_dev_cotton, L_mid_cotton, 1. - (L_ini_cotton + L_dev_cotton + L_mid_cotton), Kc_ini_cotton, Kc_mid_cotton, Kc_end_cotton, K_min_cotton, K_max_cotton, lai)\n",
    "    Kpet_rice = construct_Kpet_vec(GS_start_rice, GS_end_rice, L_ini_rice, L_dev_rice, L_mid_rice, 1. - (L_ini_rice + L_dev_rice + L_mid_rice), Kc_ini_rice, Kc_mid_rice, Kc_end_rice, K_min_rice, K_max_rice, lai)\n",
    "    Kpet_sorghum = construct_Kpet_vec(GS_start_sorghum, GS_end_sorghum, L_ini_sorghum, L_dev_sorghum, L_mid_sorghum, 1. - (L_ini_sorghum + L_dev_sorghum + L_mid_sorghum), Kc_ini_sorghum, Kc_mid_sorghum, Kc_end_sorghum, K_min_sorghum, K_max_sorghum, lai)\n",
    "    Kpet_soybeans = construct_Kpet_vec(GS_start_soybeans, GS_end_soybeans, L_ini_soybeans, L_dev_soybeans, L_mid_soybeans, 1. - (L_ini_soybeans + L_dev_soybeans + L_mid_soybeans), Kc_ini_soybeans, Kc_mid_soybeans, Kc_end_soybeans, K_min_soybeans, K_max_soybeans, lai)\n",
    "    Kpet_wheat = construct_Kpet_vec(GS_start_wheat, GS_end_wheat, L_ini_wheat, L_dev_wheat, L_mid_wheat, 1. - (L_ini_wheat + L_dev_wheat + L_mid_wheat), Kc_ini_wheat, Kc_mid_wheat, Kc_end_wheat, K_min_wheat, K_max_wheat, lai)\n",
    "\n",
    "    other = 1. - (corn + cotton + rice + sorghum + soybeans + wheat)\n",
    "    weights = jnp.array([corn, cotton, rice, sorghum, soybeans, wheat, other])\n",
    "    Kpets = jnp.array([Kpet_corn, Kpet_cotton, Kpet_rice, Kpet_sorghum, Kpet_soybeans, Kpet_wheat, jnp.ones(365)])\n",
    "    Kpet = jnp.average(Kpets, weights = weights, axis=0)\n",
    "    \n",
    "    # params that WBM sees\n",
    "    awCap_scaled = (awCap_sand * sand) + (awCap_loamy_sand * loamy_sand) + (awCap_sandy_loam * sandy_loam) + (awCap_silt_loam * silt_loam) + (awCap_silt * silt) + (awCap_loam * loam) + (awCap_sandy_clay_loam * sandy_clay_loam) + (awCap_silty_clay_loam * silty_clay_loam) + (awCap_clay_loam * clay_loam) + (awCap_sandy_clay * sandy_clay) + (awCap_silty_clay * silty_clay) + (awCap_clay * clay)\n",
    "    wiltingp_scaled = (wiltingp_sand * sand) + (wiltingp_loamy_sand * loamy_sand) + (wiltingp_sandy_loam * sandy_loam) + (wiltingp_silt_loam * silt_loam) + (wiltingp_silt * silt) + (wiltingp_loam * loam) + (wiltingp_sandy_clay_loam * sandy_clay_loam) + (wiltingp_silty_clay_loam * silty_clay_loam) + (wiltingp_clay_loam * clay_loam) + (wiltingp_sandy_clay * sandy_clay) + (wiltingp_silty_clay * silty_clay) + (wiltingp_clay * clay)\n",
    "    alpha = 1.0 + (alpha_claycoef * clayfrac) + (alpha_sandcoef * sandfrac) + (alpha_siltcoef * siltfrac)\n",
    "    betaHBV = 1.0 + (betaHBV_claycoef * clayfrac) + (betaHBV_sandcoef * sandfrac) + (betaHBV_siltcoef * siltfrac) + (betaHBV_elevcoef * elev_std)\n",
    "    \n",
    "    params = (Ts, Tm, wiltingp_scaled, awCap_scaled, rootDepth, alpha, betaHBV)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = wbm_jax(\n",
    "        tas,\n",
    "        prcp, \n",
    "        Kpet,\n",
    "        Ws_init,\n",
    "        Wi_init,\n",
    "        Sp_init,\n",
    "        lai,\n",
    "        lats,\n",
    "        params\n",
    "    )\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356af637-5581-4b69-b702-6f2b6aa6cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(theta, constants, x_forcing_nt, x_forcing_nyrs, x_maps):\n",
    "    # Read inputs\n",
    "    tas, prcp = x_forcing_nt\n",
    "    lai = x_forcing_nyrs\n",
    "    \n",
    "    awCap, wiltingp, \\\n",
    "    Ws_init, \\\n",
    "    clayfrac, sandfrac, siltfrac, \\\n",
    "    lats, elev_std, \\\n",
    "    corn, cotton, rice, sorghum, soybeans, wheat \\\n",
    "    = x_maps\n",
    "\n",
    "    # Define all constants\n",
    "    Ts, Tm, Wi_init, Sp_init = constants \n",
    "    \n",
    "    # Define all params\n",
    "        # awCap_claycoef, awCap_sandcoef, awCap_siltcoef, \\\n",
    "    # wiltingp_claycoef, wiltingp_sandcoef, wiltingp_siltcoef, \\\n",
    "    \n",
    "    awCap_scalar, wiltingp_scalar, \\\n",
    "    alpha_claycoef, alpha_sandcoef, alpha_siltcoef, \\\n",
    "    betaHBV_claycoef, betaHBV_sandcoef, betaHBV_siltcoef, betaHBV_elevcoef, \\\n",
    "    GS_start_corn, GS_end_corn, L_ini_corn, L_dev_corn, L_mid_corn, Kc_ini_corn, Kc_mid_corn, Kc_end_corn, K_min_corn, K_max_corn, \\\n",
    "    GS_start_cotton, GS_end_cotton, L_ini_cotton, L_dev_cotton, L_mid_cotton, Kc_ini_cotton, Kc_mid_cotton, Kc_end_cotton, K_min_cotton, K_max_cotton, \\\n",
    "    GS_start_rice, GS_end_rice, L_ini_rice, L_dev_rice, L_mid_rice, Kc_ini_rice, Kc_mid_rice, Kc_end_rice, K_min_rice, K_max_rice,  \\\n",
    "    GS_start_sorghum, GS_end_sorghum, L_ini_sorghum, L_dev_sorghum, L_mid_sorghum, Kc_ini_sorghum, Kc_mid_sorghum, Kc_end_sorghum, K_min_sorghum, K_max_sorghum, \\\n",
    "    GS_start_soybeans, GS_end_soybeans, L_ini_soybeans, L_dev_soybeans, L_mid_soybeans, Kc_ini_soybeans, Kc_mid_soybeans, Kc_end_soybeans, K_min_soybeans, K_max_soybeans, \\\n",
    "    GS_start_wheat, GS_end_wheat, L_ini_wheat, L_dev_wheat, L_mid_wheat, Kc_ini_wheat, Kc_mid_wheat, Kc_end_wheat, K_min_wheat, K_max_wheat \\\n",
    "    = jnp.exp(theta)\n",
    "\n",
    "    # Construct Kpet as weighted average\n",
    "    Kpet_corn = construct_Kpet_vec(GS_start_corn, GS_end_corn, L_ini_corn, L_dev_corn, L_mid_corn, 1. - (L_ini_corn + L_dev_corn + L_mid_corn), Kc_ini_corn, Kc_mid_corn, Kc_end_corn, K_min_corn, K_max_corn, lai)\n",
    "    Kpet_cotton = construct_Kpet_vec(GS_start_cotton, GS_end_cotton, L_ini_cotton, L_dev_cotton, L_mid_cotton, 1. - (L_ini_cotton + L_dev_cotton + L_mid_cotton), Kc_ini_cotton, Kc_mid_cotton, Kc_end_cotton, K_min_cotton, K_max_cotton, lai)\n",
    "    Kpet_rice = construct_Kpet_vec(GS_start_rice, GS_end_rice, L_ini_rice, L_dev_rice, L_mid_rice, 1. - (L_ini_rice + L_dev_rice + L_mid_rice), Kc_ini_rice, Kc_mid_rice, Kc_end_rice, K_min_rice, K_max_rice, lai)\n",
    "    Kpet_sorghum = construct_Kpet_vec(GS_start_sorghum, GS_end_sorghum, L_ini_sorghum, L_dev_sorghum, L_mid_sorghum, 1. - (L_ini_sorghum + L_dev_sorghum + L_mid_sorghum), Kc_ini_sorghum, Kc_mid_sorghum, Kc_end_sorghum, K_min_sorghum, K_max_sorghum, lai)\n",
    "    Kpet_soybeans = construct_Kpet_vec(GS_start_soybeans, GS_end_soybeans, L_ini_soybeans, L_dev_soybeans, L_mid_soybeans, 1. - (L_ini_soybeans + L_dev_soybeans + L_mid_soybeans), Kc_ini_soybeans, Kc_mid_soybeans, Kc_end_soybeans, K_min_soybeans, K_max_soybeans, lai)\n",
    "    Kpet_wheat = construct_Kpet_vec(GS_start_wheat, GS_end_wheat, L_ini_wheat, L_dev_wheat, L_mid_wheat, 1. - (L_ini_wheat + L_dev_wheat + L_mid_wheat), Kc_ini_wheat, Kc_mid_wheat, Kc_end_wheat, K_min_wheat, K_max_wheat, lai)\n",
    "\n",
    "    other = 1. - (corn + cotton + rice + sorghum + soybeans + wheat)\n",
    "    weights = jnp.array([corn, cotton, rice, sorghum, soybeans, wheat, other])\n",
    "    Kpets = jnp.array([Kpet_corn, Kpet_cotton, Kpet_rice, Kpet_sorghum, Kpet_soybeans, Kpet_wheat, jnp.ones(365)])\n",
    "    Kpet = jnp.average(Kpets, weights = weights, axis=0)\n",
    "    \n",
    "    # params that WBM sees\n",
    "    awCap_scaled = (awCap_scalar * awCap) #+  + (awCap_claycoef * clayfrac) + (awCap_sandcoef * sandfrac) + (awCap_siltcoef * siltfrac) # (awCap_scalar * awCap) + \n",
    "    wiltingp_scaled = (wiltingp_scalar * wiltingp) #+  + (wiltingp_claycoef * clayfrac) + (wiltingp_sandcoef * sandfrac) + (wiltingp_siltcoef * siltfrac) # (wiltingp_scalar * wiltingp) + \n",
    "    alpha = 1.0 + (alpha_claycoef * clayfrac) + (alpha_sandcoef * sandfrac) + (alpha_siltcoef * siltfrac)\n",
    "    betaHBV = 1.0 + (betaHBV_claycoef * clayfrac) + (betaHBV_sandcoef * sandfrac) + (betaHBV_siltcoef * siltfrac) + (betaHBV_elevcoef * elev_std)\n",
    "    \n",
    "    params = (Ts, Tm, wiltingp_scaled, awCap_scaled, alpha, betaHBV)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = wbm_jax(\n",
    "        tas,\n",
    "        prcp, \n",
    "        Kpet,\n",
    "        Ws_init,\n",
    "        Wi_init,\n",
    "        Sp_init,\n",
    "        lai,\n",
    "        lats,\n",
    "        params\n",
    "    )\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90258cb5-f985-468b-8ca7-89e434f3f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_store(subset_name, obs_name, _error_fn, error_fn_name, n_epochs,\n",
    "                    batch_size = 2**7,\n",
    "                    opt = 'adam',\n",
    "                    learning_rate = 1e-2,\n",
    "                    val_frac = 0.2,\n",
    "                    reg_const = 0.01,\n",
    "                    initial_params = initial_params,\n",
    "                    params_lower = params_lower,\n",
    "                    params_upper = params_upper):\n",
    "    #############################################\n",
    "    # Loss function with correct error metric\n",
    "    ############################################\n",
    "    # Prediction loss\n",
    "    def prediction_loss(theta, constants,\n",
    "                        x_forcing_nt, x_forcing_nyrs, x_maps, ys):\n",
    "        \n",
    "        prediction = make_prediction_vic(theta, constants, x_forcing_nt, x_forcing_nyrs, x_maps)\n",
    "        \n",
    "        return _error_fn(prediction, ys)\n",
    "    \n",
    "    # Regularization loss\n",
    "    def reg_loss(theta, initial_params, params_lower, params_upper):\n",
    "        \n",
    "        return jnp.nansum((theta - initial_params)**2 / ((theta - params_lower) * (params_upper - theta)))\n",
    "    \n",
    "    # Total loss\n",
    "    def loss_fn(theta, reg_const, initial_params, params_lower, params_upper, constants,\n",
    "                x_forcing_nt, x_forcing_nyrs, x_maps, ys):\n",
    "        \n",
    "        return prediction_loss(theta, constants, x_forcing_nt, x_forcing_nyrs, x_maps, ys) + \\\n",
    "                reg_const * reg_loss(theta, initial_params, params_lower, params_upper)\n",
    "    \n",
    "    # jit and vmap it\n",
    "    pred_loss_value = jax.jit(jax.vmap(prediction_loss, in_axes=(None, None, 0, 0, 0, 0), out_axes=0))\n",
    "    loss_value_and_grad = jax.jit(jax.vmap(jax.value_and_grad(loss_fn), in_axes=(None, None, None, None, None, None, 0, 0, 0, 0), out_axes=0))\n",
    "\n",
    "    ###########################\n",
    "    # Setup\n",
    "    ###########################\n",
    "    # Read data\n",
    "    ys, x_forcing_nt, x_forcing_nyrs, x_maps = read_inputs_vic(subset_name, obs_name, True)\n",
    "    N = ys.shape[0]\n",
    "    \n",
    "    # Get train/val split over space\n",
    "    N_val = int(N * val_frac)\n",
    "    N_train = N - N_val\n",
    "    \n",
    "    train_idx = np.random.choice(N, N_train, replace=False)\n",
    "    ys_train, x_forcing_nt_train, x_forcing_nyrs_train, x_maps_train = ys[train_idx], x_forcing_nt[train_idx], x_forcing_nyrs[train_idx], x_maps[train_idx]\n",
    "\n",
    "    if N_val > 0:\n",
    "        val_idx = np.array([n for n in np.arange(N) if n not in train_idx])\n",
    "        ys_val, x_forcing_nt_val, x_forcing_nyrs_val, x_maps_val = ys[val_idx], x_forcing_nt[val_idx], x_forcing_nyrs[val_idx], x_maps[val_idx]\n",
    "    \n",
    "    # Define mini-batch hyper-parameters\n",
    "    # n_minibatches = 1 + N // batch_size\n",
    "    n_minibatches = 1 + N_train // batch_size\n",
    "\n",
    "    # Initial parameters\n",
    "    theta = np.random.uniform(low=params_lower, high=params_upper)\n",
    "    # theta = np.random.normal(loc=initial_params, scale=abs(initial_params/10.))\n",
    "    # theta = initial_params\n",
    "\n",
    "    # Optimizer\n",
    "    if opt == 'adam':\n",
    "        adam = optax.adam(learning_rate=learning_rate)\n",
    "        opt_fn = adam.update\n",
    "        opt_state = adam.init(theta)\n",
    "    elif opt == 'sgd':\n",
    "        learning_rate = 1e-5\n",
    "        opt_state = None\n",
    "        def sgd(gradients, state):\n",
    "            return -learning_rate * gradients, state\n",
    "        opt_fn = sgd\n",
    "\n",
    "    # Loss\n",
    "    train_loss_out = np.empty(n_epochs + 1)\n",
    "    pred_loss_out = np.empty(n_epochs + 1)\n",
    "    reg_loss_out = np.empty(n_epochs + 1)\n",
    "    val_loss_out = np.empty(n_epochs + 1)\n",
    "\n",
    "    # Where to store results\n",
    "    datetime_str = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "    training_name = f\"{error_fn_name}_{str(n_epochs)}epochs_{str(batch_size)}batchsize_{opt}-opt_{str(val_frac)}val_{str(reg_const)}reg_{datetime_str}\"\n",
    "\n",
    "    out_file_path = f\"{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/training_res/{training_name}.txt\"\n",
    "    f = open(out_file_path, \"w\")\n",
    "    f.write(f\"epoch metric train_loss pred_loss reg_loss val_loss {' '.join(param_names_vic)}\\n\")\n",
    "\n",
    "    # initial results\n",
    "    pred_loss_init = jnp.mean(pred_loss_value(theta,\n",
    "                                              constants,\n",
    "                                              x_forcing_nt_train,\n",
    "                                              x_forcing_nyrs_train,\n",
    "                                              x_maps_train,\n",
    "                                              ys_train))\n",
    "    if N_val > 0:\n",
    "        val_loss_init = jnp.mean(pred_loss_value(theta,\n",
    "                                                 constants,\n",
    "                                                 x_forcing_nt_val,\n",
    "                                                 x_forcing_nyrs_val,\n",
    "                                                 x_maps_val,\n",
    "                                                 ys_val))\n",
    "    else:\n",
    "        val_loss_init = np.nan\n",
    "        \n",
    "    reg_loss_init = reg_loss(theta, initial_params, params_lower, params_upper)\n",
    "    print(f\"Epoch 0 pred loss: {pred_loss_init:.4f}, reg_loss: {reg_loss_init:.4f}, val loss: {val_loss_init:.4f}\")\n",
    "    \n",
    "    ###########################\n",
    "    # Training loop\n",
    "    ###########################\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        # Shuffle indices\n",
    "        shuffled_inds = np.random.permutation(N_train)\n",
    "    \n",
    "        # Generate a mini-batch\n",
    "        minibatch_inds = [shuffled_inds[(i*batch_size):((i + 1)*batch_size)] for i in range(n_minibatches)]\n",
    "\n",
    "        # For batch loss\n",
    "        batch_loss = [None] * n_minibatches\n",
    "\n",
    "        for idx, inds in enumerate(minibatch_inds):\n",
    "            # Calculate gradient of loss function, update parameters\n",
    "            loss, grads = loss_value_and_grad(theta, reg_const, initial_params, params_lower, params_upper, constants,\n",
    "                                              x_forcing_nt_train[inds],\n",
    "                                              x_forcing_nyrs_train[inds],\n",
    "                                              x_maps_train[inds],\n",
    "                                              ys_train[inds])\n",
    "            updates, opt_state = opt_fn(jnp.nanmean(grads, axis=0), opt_state)\n",
    "            theta = optax.apply_updates(theta, updates)\n",
    "            batch_loss[idx] = loss\n",
    "            # Break if theta steps outside bounds\n",
    "            if (theta < params_lower).any() or (theta > params_upper).any():\n",
    "                print('Found invalid parameter')\n",
    "                f.close()\n",
    "                # os.remove(out_file_path)\n",
    "                return train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta\n",
    "\n",
    "        # Save all losses\n",
    "        train_loss_out[epoch] = jnp.nanmean(jnp.array([item for row in batch_loss for item in row]))\n",
    "        reg_loss_out[epoch] = reg_loss(theta, initial_params, params_lower, params_upper)\n",
    "        pred_loss_out[epoch] = train_loss_out[epoch] - (reg_const * reg_loss_out[epoch])\n",
    "        if N_val > 0:\n",
    "            val_loss_out[epoch] = jnp.mean(pred_loss_value(theta,\n",
    "                                                          constants,\n",
    "                                                          x_forcing_nt_val,\n",
    "                                                          x_forcing_nyrs_val,\n",
    "                                                          x_maps_val,\n",
    "                                                          ys_val))\n",
    "        else:\n",
    "            val_loss_out[epoch] = jnp.nan\n",
    "        \n",
    "        # Write every epoch\n",
    "        theta_str = [str(param) for param in theta]\n",
    "        f.write(f\"{str(epoch + 1)} {error_fn_name} {train_loss_out[epoch]:.4f} {pred_loss_out[epoch]:.4f} {reg_loss_out[epoch]:.4f} {val_loss_out[epoch]:.4f} {' '.join(theta_str)}\\n\")\n",
    "        # Print every 5\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {str(epoch + 1)} total loss: {train_loss_out[epoch]:.4f}, pred loss: {pred_loss_out[epoch]:.4f}, reg_loss: {reg_loss_out[epoch]:.4f}, val loss: {val_loss_out[epoch]:.4f}\")\n",
    "\n",
    "    f.close()\n",
    "    return train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7849bfd0-5f4e-4402-8408-9fbc4cce6f0a",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbfca15-2fec-4ca2-883d-75d8efc90cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of timeseries needed for quantile RMSE\n",
    "N = 2555\n",
    "\n",
    "# Define all error functions\n",
    "# RMSE\n",
    "_rmse = lambda prediction, ys: jnp.sqrt(jnp.nanmean((prediction-ys)**2))\n",
    "\n",
    "# MSE\n",
    "_mse = lambda prediction, ys: jnp.nanmean((prediction-ys)**2)\n",
    "\n",
    "# KGE\n",
    "def _kge(prediction, ys):\n",
    "    corr = jnp.nanmean((prediction - jnp.nanmean(prediction))*(ys - jnp.nanmean(ys))) / (jnp.nanstd(prediction) * jnp.nanstd(ys))\n",
    "    mean_ratio = jnp.nanmean(prediction) / jnp.nanmean(ys)\n",
    "    std_ratio = jnp.nanstd(prediction) / jnp.nanstd(ys)\n",
    "    kge = 1 - jnp.sqrt((corr - 1)**2 + (mean_ratio - 1)**2 + (std_ratio - 1)**2)\n",
    "    return -kge \n",
    "\n",
    "# q0-25 RMSE\n",
    "qmax = 0.25\n",
    "size = round(N * qmax)\n",
    "def _q25rmse(prediction, ys):\n",
    "    thresh = jnp.quantile(ys, qmax)\n",
    "    inds = jnp.where(ys <= thresh, size=size)\n",
    "    prediction_q = prediction[inds]\n",
    "    ys_q = ys[inds]\n",
    "    return jnp.sqrt(jnp.nanmean((prediction_q - ys_q)**2))\n",
    "\n",
    "# q75-100 RMSE\n",
    "qmin = 0.75\n",
    "size = round(N * qmin)\n",
    "def _q75rmse(prediction, ys):\n",
    "    thresh = jnp.quantile(ys, qmin)\n",
    "    inds = jnp.where(ys >= thresh, size=size)\n",
    "    prediction_q = prediction[inds]\n",
    "    ys_q = ys[inds]\n",
    "    return jnp.sqrt(jnp.nanmean((prediction_q - ys_q)**2))\n",
    "\n",
    "_error_fns = [_rmse, _mse, _kge, _q25rmse, _q75rmse]\n",
    "error_fn_names = ['rmse', 'mse', 'kge', 'q0-25rmse', 'q75-100rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746119c-d74f-4f54-a3d3-62809522617a",
   "metadata": {},
   "source": [
    "### SMAP, VIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d66c3bf2-ed26-4d19-bdde-66530af5c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "subset_name = 'centralUS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ccef39-b939-40ad-b694-0cd667d7a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Parallelize with dask delayed\n",
    "# delayed = []\n",
    "\n",
    "# for obs_name in ['VIC']:\n",
    "#     for _error_fn, error_fn_name in zip(_error_fns, error_fn_names):\n",
    "#         # Hyperparameter adjustments\n",
    "#         reg_const = 0.001\n",
    "#         learning_rate = 1e-3\n",
    "        \n",
    "#         for batch_size in [2**5, 2**6, 2**7, 2**8, 2**9]:\n",
    "#             delayed.append(dask.delayed(train_and_store)(subset_name = subset_name,\n",
    "#                                                          obs_name = obs_name,\n",
    "#                                                          _error_fn = _error_fn,\n",
    "#                                                          error_fn_name = error_fn_name,\n",
    "#                                                          batch_size = batch_size,\n",
    "#                                                          reg_const = reg_const,\n",
    "#                                                          learning_rate = learning_rate,\n",
    "#                                                          n_epochs = 30,\n",
    "#                                                          val_frac = 0.0))\n",
    "\n",
    "# out = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78298f-5adc-4abe-9c51-0e8d4699cc6e",
   "metadata": {},
   "source": [
    "### VIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b521f9df-98f6-48a2-aa33-a8c015979e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "subset_name = 'centralUS'\n",
    "obs_name = 'VIC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349d5a80-eb75-4f6b-a28e-5120215ada8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 151.2948, reg_loss: 210.8976, val loss: nan\n",
      "Epoch 1 total loss: 145.2396, pred loss: 144.4348, reg_loss: 80.4860, val loss: nan\n",
      "Epoch 6 total loss: 69.4841, pred loss: 69.0368, reg_loss: 44.7317, val loss: nan\n",
      "Epoch 11 total loss: 43.8797, pred loss: 43.6616, reg_loss: 21.8073, val loss: nan\n",
      "Epoch 16 total loss: 33.9699, pred loss: 33.7160, reg_loss: 25.3886, val loss: nan\n",
      "Epoch 21 total loss: 29.3166, pred loss: 29.0769, reg_loss: 23.9760, val loss: nan\n",
      "Epoch 26 total loss: 26.9986, pred loss: 26.7692, reg_loss: 22.9425, val loss: nan\n",
      "Epoch 31 total loss: 26.6741, pred loss: 26.4396, reg_loss: 23.4555, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# soil class scalar using log, random start, scaled\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ab6a6-8fb7-4278-89b5-36201d682b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb46dd6-c971-4c2a-a6fe-7b41afec608f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb1b7ab2-0ffd-4dfb-8e0e-315c78e7be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 193.0402, reg_loss: 275.4830, val loss: nan\n",
      "Epoch 1 total loss: 188.6235, pred loss: 187.8289, reg_loss: 79.4585, val loss: nan\n",
      "Epoch 6 total loss: 126.1107, pred loss: 125.7390, reg_loss: 37.1648, val loss: nan\n",
      "Epoch 11 total loss: 72.8450, pred loss: 72.5604, reg_loss: 28.4649, val loss: nan\n",
      "Epoch 16 total loss: 46.6197, pred loss: 46.3994, reg_loss: 22.0373, val loss: nan\n",
      "Epoch 21 total loss: 37.7458, pred loss: 37.4790, reg_loss: 26.6730, val loss: nan\n",
      "Epoch 26 total loss: 33.6993, pred loss: 33.4204, reg_loss: 27.8836, val loss: nan\n",
      "Epoch 31 total loss: 32.5981, pred loss: 32.3313, reg_loss: 26.6761, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# soil class scalar using log, random start\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14e21297-78f4-4e48-9d83-e56fdcd9ead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 135.3471, reg_loss: 619.0754, val loss: nan\n",
      "Epoch 1 total loss: 117.6131, pred loss: 116.4815, reg_loss: 113.1661, val loss: nan\n",
      "Epoch 6 total loss: 51.9875, pred loss: 51.4211, reg_loss: 56.6422, val loss: nan\n",
      "Epoch 11 total loss: 42.1469, pred loss: 41.7205, reg_loss: 42.6382, val loss: nan\n",
      "Epoch 16 total loss: 38.3026, pred loss: 37.9357, reg_loss: 36.6916, val loss: nan\n",
      "Epoch 21 total loss: 36.0634, pred loss: 35.7291, reg_loss: 33.4250, val loss: nan\n",
      "Epoch 26 total loss: 34.8380, pred loss: 34.5312, reg_loss: 30.6831, val loss: nan\n",
      "Epoch 31 total loss: 34.2670, pred loss: 33.9823, reg_loss: 28.4617, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# NOAH + regression on soil contents using log, random start\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f507a9f5-188a-4a31-8124-3009179601fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 123.8417, reg_loss: 195.7297, val loss: nan\n",
      "Epoch 1 total loss: 112.4669, pred loss: 111.8620, reg_loss: 60.4960, val loss: nan\n",
      "Epoch 6 total loss: 64.0041, pred loss: 63.6166, reg_loss: 38.7476, val loss: nan\n",
      "Epoch 11 total loss: 48.7567, pred loss: 48.4685, reg_loss: 28.8285, val loss: nan\n",
      "Epoch 16 total loss: 44.0785, pred loss: 43.8326, reg_loss: 24.5905, val loss: nan\n",
      "Epoch 21 total loss: 41.5831, pred loss: 41.3319, reg_loss: 25.1146, val loss: nan\n",
      "Epoch 26 total loss: 39.6683, pred loss: 39.4182, reg_loss: 25.0046, val loss: nan\n",
      "Epoch 31 total loss: 38.1857, pred loss: 37.9466, reg_loss: 23.9035, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# NOAH, log, random start\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620a8ab1-74c4-4f45-8eb2-8b9c3f8cbb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 139.6269, reg_loss: 0.0000, val loss: nan\n",
      "Epoch 1 total loss: 115.5750, pred loss: 115.4117, reg_loss: 16.3227, val loss: nan\n",
      "Epoch 6 total loss: 39.4394, pred loss: 39.2534, reg_loss: 18.6035, val loss: nan\n",
      "Epoch 11 total loss: 36.1339, pred loss: 35.9304, reg_loss: 20.3449, val loss: nan\n",
      "Epoch 16 total loss: 34.4622, pred loss: 34.2524, reg_loss: 20.9795, val loss: nan\n",
      "Epoch 21 total loss: 33.8736, pred loss: 33.6661, reg_loss: 20.7497, val loss: nan\n",
      "Epoch 26 total loss: 33.6492, pred loss: 33.4455, reg_loss: 20.3705, val loss: nan\n",
      "Epoch 31 total loss: 33.4985, pred loss: 33.2909, reg_loss: 20.7576, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# NOAH + regression on soil contents using log\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ba39f1-4283-4075-b3ed-00a3e13ec6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 398.7293, reg_loss: 149.0950, val loss: nan\n",
      "Epoch 1 total loss: 357.0240, pred loss: 356.3212, reg_loss: 70.2793, val loss: nan\n",
      "Epoch 6 total loss: 109.7697, pred loss: 109.0889, reg_loss: 68.0762, val loss: nan\n",
      "Epoch 11 total loss: 55.9770, pred loss: 55.7513, reg_loss: 22.5671, val loss: nan\n",
      "Epoch 16 total loss: 46.4427, pred loss: 46.3075, reg_loss: 13.5284, val loss: nan\n",
      "Epoch 21 total loss: 41.7153, pred loss: 41.5609, reg_loss: 15.4350, val loss: nan\n",
      "Epoch 26 total loss: 39.8752, pred loss: 39.7038, reg_loss: 17.1428, val loss: nan\n",
      "Epoch 31 total loss: 39.2561, pred loss: 39.0825, reg_loss: 17.3580, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Regression on soil contents plus intercept using log, random start\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77d9231b-6c57-48b9-8f63-13dc7f8faf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 56.6325, reg_loss: 0.0000, val loss: nan\n",
      "Epoch 1 total loss: 49.9408, pred loss: 49.8898, reg_loss: 5.1048, val loss: nan\n",
      "Epoch 6 total loss: 40.4870, pred loss: 40.3236, reg_loss: 16.3395, val loss: nan\n",
      "Epoch 11 total loss: 39.0435, pred loss: 38.8532, reg_loss: 19.0305, val loss: nan\n",
      "Epoch 16 total loss: 38.5324, pred loss: 38.3486, reg_loss: 18.3766, val loss: nan\n",
      "Epoch 21 total loss: 38.1424, pred loss: 37.9535, reg_loss: 18.8949, val loss: nan\n",
      "Epoch 26 total loss: 37.8405, pred loss: 37.6562, reg_loss: 18.4250, val loss: nan\n",
      "Epoch 31 total loss: 37.6220, pred loss: 37.4392, reg_loss: 18.2792, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Regression on soil contents plus intercept using log\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c994d7-6090-459a-8da4-0e7d800be859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 56.2360, reg_loss: 0.0000, val loss: nan\n",
      "Epoch 1 total loss: 49.7308, pred loss: 49.6789, reg_loss: 5.1875, val loss: nan\n",
      "Epoch 6 total loss: 40.0404, pred loss: 39.8642, reg_loss: 17.6196, val loss: nan\n",
      "Epoch 11 total loss: 39.1196, pred loss: 38.9322, reg_loss: 18.7309, val loss: nan\n",
      "Epoch 16 total loss: 38.4627, pred loss: 38.2776, reg_loss: 18.5110, val loss: nan\n",
      "Epoch 21 total loss: 37.9999, pred loss: 37.8117, reg_loss: 18.8236, val loss: nan\n",
      "Epoch 26 total loss: 37.7233, pred loss: 37.5467, reg_loss: 17.6600, val loss: nan\n",
      "Epoch 31 total loss: 37.5629, pred loss: 37.3785, reg_loss: 18.4447, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Regression on soil contents using log\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b74bf2b8-75a0-4512-bae7-9bbd3ca72e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 pred loss: 56.2178, reg_loss: 0.0000, val loss: nan\n",
      "Epoch 1 total loss: 54.7293, pred loss: 54.6346, reg_loss: 9.4750, val loss: nan\n",
      "Epoch 6 total loss: 46.0005, pred loss: 45.8825, reg_loss: 11.7977, val loss: nan\n",
      "Epoch 11 total loss: 45.2821, pred loss: 45.1470, reg_loss: 13.5068, val loss: nan\n",
      "Epoch 16 total loss: 44.7553, pred loss: 44.6166, reg_loss: 13.8699, val loss: nan\n",
      "Epoch 21 total loss: 44.3233, pred loss: 44.1600, reg_loss: 16.3280, val loss: nan\n",
      "Epoch 26 total loss: 43.9569, pred loss: 43.7966, reg_loss: 16.0265, val loss: nan\n",
      "Epoch 31 total loss: 43.6540, pred loss: 43.4892, reg_loss: 16.4812, val loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Regression on soil contents using non-log\n",
    "_error_fn = _rmse\n",
    "error_fn_name = 'rmse'\n",
    "\n",
    "train_loss_out, pred_loss_out, reg_loss_out, val_loss_out, theta = train_and_store(subset_name = subset_name,\n",
    "                                                                                   obs_name = obs_name,\n",
    "                                                                                   _error_fn = _error_fn,\n",
    "                                                                                   error_fn_name = error_fn_name,\n",
    "                                                                                   learning_rate = 0.001,\n",
    "                                                                                   n_epochs = 30,\n",
    "                                                                                   val_frac = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9230a-d22a-4317-ae66-ef84df102e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c10b78-78ac-4912-b5f7-daba0f967211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Parallelize with dask delayed\n",
    "# delayed = []\n",
    "\n",
    "# for _error_fn, error_fn_name in zip(_error_fns, error_fn_names):\n",
    "#     # Hyperparameter adjustments\n",
    "#     if error_fn_name == 'kge':\n",
    "#         reg_const = 0.001\n",
    "#     else:\n",
    "#         reg_const = 0.01\n",
    "\n",
    "#     if error_fn_name == 'mse':\n",
    "#         learning_rate = 1e-3\n",
    "#     else:\n",
    "#         learning_rate = 1e-2\n",
    "        \n",
    "#     for batch_size in [2**5, 2**6, 2**7, 2**8, 2**9]:\n",
    "#         delayed.append(dask.delayed(train_and_store)(subset_name = subset_name,\n",
    "#                                                      obs_name = obs_name,\n",
    "#                                                      _error_fn = _error_fn,\n",
    "#                                                      error_fn_name = error_fn_name,\n",
    "#                                                      batch_size = batch_size,\n",
    "#                                                      reg_const = reg_const,\n",
    "#                                                      learning_rate = learning_rate,\n",
    "#                                                      n_epochs = 30,\n",
    "#                                                      val_frac = 0.0))\n",
    "\n",
    "# out = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f30aa-6bdc-491c-9efb-4adcf412fcde",
   "metadata": {},
   "source": [
    "### MOSAIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34239120-98d7-44a9-be56-2061e53f8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "subset_name = 'centralUS'\n",
    "obs_name = 'MOSAIC'\n",
    "\n",
    "# needed for quantile RMSE\n",
    "N = np.load(f'{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/{obs_name}_validation.npy').shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f882d851-9169-4706-b866-fb52b3c33c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 23s, sys: 20.8 s, total: 4min 43s\n",
      "Wall time: 56min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parallelize with dask delayed\n",
    "delayed = []\n",
    "\n",
    "for _error_fn, error_fn_name in zip(_error_fns, error_fn_names):\n",
    "    # Hyperparameter adjustments\n",
    "    if error_fn_name == 'kge':\n",
    "        reg_const = 0.001\n",
    "    else:\n",
    "        reg_const = 0.01\n",
    "\n",
    "    if error_fn_name == 'mse':\n",
    "        learning_rate = 1e-3\n",
    "    else:\n",
    "        learning_rate = 1e-2\n",
    "        \n",
    "    for batch_size in [2**5, 2**6, 2**7, 2**8, 2**9]:\n",
    "        delayed.append(dask.delayed(train_and_store)(subset_name = subset_name,\n",
    "                                                     obs_name = obs_name,\n",
    "                                                     _error_fn = _error_fn,\n",
    "                                                     error_fn_name = error_fn_name,\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     reg_const = reg_const,\n",
    "                                                     learning_rate = learning_rate,\n",
    "                                                     n_epochs = 30,\n",
    "                                                     val_frac = 0.0))\n",
    "\n",
    "out = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dac970-d6d5-454a-a595-7c7d44487e61",
   "metadata": {},
   "source": [
    "### NOAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a960759-af42-4edb-8066-ad431f90c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "subset_name = 'centralUS'\n",
    "obs_name = 'NOAH'\n",
    "\n",
    "# needed for quantile RMSE\n",
    "N = np.load(f'{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/{obs_name}_validation.npy').shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e9352-6283-411a-97ec-ce4866781bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parallelize with dask delayed\n",
    "delayed = []\n",
    "\n",
    "for _error_fn, error_fn_name in zip(_error_fns, error_fn_names):\n",
    "    # Hyperparameter adjustments\n",
    "    if error_fn_name == 'kge':\n",
    "        reg_const = 0.001\n",
    "    else:\n",
    "        reg_const = 0.01\n",
    "\n",
    "    if error_fn_name == 'mse':\n",
    "        learning_rate = 1e-3\n",
    "    else:\n",
    "        learning_rate = 1e-2\n",
    "        \n",
    "    for batch_size in [2**5, 2**6, 2**7, 2**8, 2**9]:\n",
    "        delayed.append(dask.delayed(train_and_store)(subset_name = subset_name,\n",
    "                                                     obs_name = obs_name,\n",
    "                                                     _error_fn = _error_fn,\n",
    "                                                     error_fn_name = error_fn_name,\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     reg_const = reg_const,\n",
    "                                                     learning_rate = learning_rate,\n",
    "                                                     n_epochs = 30,\n",
    "                                                     val_frac = 0.0))\n",
    "\n",
    "out = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872df4b-4415-4b2d-a666-2b98dd230e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137beaed-9ad8-4da4-bade-d02819c31ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
