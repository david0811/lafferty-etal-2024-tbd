{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58728ec0-f35e-4ff7-aeb9-8cfa3bfead9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "import dask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from water_balance_jax import wbm_jax, construct_Kpet_vec\n",
    "from initial_params import initial_params, constants\n",
    "from param_bounds import params_lower, params_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dd5a8a-c642-45c7-a747-4d94d0542cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#### Directories ####\n",
    "#####################\n",
    "project_data_path = \"/storage/group/pches/default/users/dcl5300/wbm_soilM_crop_uc_lafferty-etal-2024-tbd_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f6e822-4384-416d-8179-17c4a908141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter names\n",
    "param_names = [\n",
    "\"awCap_scalar\", \"wiltingp_scalar\", \\\n",
    "\"alpha_claycoef\", \"alpha_sandcoef\", \"alpha_siltcoef\", \\\n",
    "\"betaHBV_claycoef\", \"betaHBV_sandcoef\", \"betaHBV_siltcoef\", \"betaHBV_elevcoef\", \\\n",
    "\"GS_start_corn\", \"GS_end_corn\", \"L_ini_corn\", \"L_dev_corn\", \"L_mid_corn\", \"Kc_ini_corn\", \"Kc_mid_corn\", \"Kc_end_corn\", \"K_min_corn\", \"K_max_corn\", \\\n",
    "\"GS_start_cotton\", \"GS_end_cotton\", \"L_ini_cotton\", \"L_dev_cotton\", \"L_mid_cotton\", \"Kc_ini_cotton\", \"Kc_mid_cotton\", \"Kc_end_cotton\", \"K_min_cotton\", \"K_max_cotton\", \\\n",
    "\"GS_start_rice\", \"GS_end_rice\", \"L_ini_rice\", \"L_dev_rice\", \"L_mid_rice\", \"Kc_ini_rice\", \"Kc_mid_rice\", \"Kc_end_rice\", \"K_min_rice\", \"K_max_rice\", \\\n",
    "\"GS_start_sorghum\", \"GS_end_sorghum\", \"L_ini_sorghum\", \"L_dev_sorghum\", \"L_mid_sorghum\", \"Kc_ini_sorghum\", \"Kc_mid_sorghum\", \"Kc_end_sorghum\", \"K_min_sorghum\", \"K_max_sorghum\",\\\n",
    "\"GS_start_soybeans\", \"GS_end_soybeans\", \"L_ini_soybeans\", \"L_dev_soybeans\", \"L_mid_soybeans\", \"Kc_ini_soybeans\", \"Kc_mid_soybeans\", \"Kc_end_soybeans\", \"K_min_soybeans\", \"K_max_soybeans\", \\\n",
    "\"GS_start_wheat\", \"GS_end_wheat\", \"L_ini_wheat\", \"L_dev_wheat\", \"L_mid_wheat\", \"Kc_ini_wheat\", \"Kc_mid_wheat\", \"Kc_end_wheat\", \"K_min_wheat\", \"K_max_wheat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7255bb-79c3-4127-8310-a59a85367a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-1ceb38d4-c7a0-11ee-830d-00001029fe80</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.SLURMCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">SLURMCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">4a322ec6</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-5ada7642-5643-45d0-9234-bde811d5f699</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://10.6.0.162:37507\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.6.0.162:37507' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "### Dask ###\n",
    "############\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    # account=\"pches\",\n",
    "    account=\"open\",\n",
    "    cores=1,\n",
    "    memory=\"10GiB\",\n",
    "    walltime=\"00:30:00\"\n",
    ")\n",
    "cluster.scale(jobs=30)  # ask for jobs\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a624d-541f-4495-b2dc-17d804a8b3c0",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab53d502-b533-4e02-8cda-3f0431b901c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inputs(subset_name, obs_name, remove_nans):\n",
    "    ######################\n",
    "    # Read obs\n",
    "    obs = np.load(f'{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/{obs_name}_validation.npy')\n",
    "\n",
    "    ######################\n",
    "    # Read and extract inputs\n",
    "    npz = np.load(f\"{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/inputs.npz\")\n",
    "\n",
    "    # Meteo forcing\n",
    "    tas = npz['tas']\n",
    "    prcp = npz['prcp']\n",
    "\n",
    "    # LAI\n",
    "    lai = npz['lai']\n",
    "\n",
    "    # Soil properties\n",
    "    awCap = npz['awCap']\n",
    "    wiltingp = npz['wiltingp']\n",
    "    clayfrac = npz['clayfrac']\n",
    "    sandfrac = npz['sandfrac']\n",
    "    siltfrac = npz['siltfrac']\n",
    "\n",
    "    # Land use\n",
    "    corn = npz['corn']\n",
    "    cotton = npz['cotton']\n",
    "    rice = npz['rice']\n",
    "    sorghum = npz['sorghum']\n",
    "    soybeans = npz['soybeans']\n",
    "    durum_wheat = npz['durum_wheat']\n",
    "    spring_wheat = npz['spring_wheat']\n",
    "    winter_wheat = npz['winter_wheat']\n",
    "    wheat = durum_wheat + spring_wheat + winter_wheat\n",
    "    \n",
    "    cropland_other = npz['cropland_other']\n",
    "    water = npz['water']\n",
    "    evergreen_needleleaf = npz['evergreen_needleleaf']\n",
    "    evergreen_broadleaf = npz['evergreen_broadleaf']\n",
    "    deciduous_needleleaf = npz['deciduous_needleleaf']\n",
    "    deciduous_broadleaf = npz['deciduous_broadleaf']\n",
    "    mixed_forest = npz['mixed_forest']\n",
    "    woodland = npz['woodland']\n",
    "    wooded_grassland = npz['wooded_grassland']\n",
    "    closed_shurbland = npz['closed_shurbland']\n",
    "    open_shrubland = npz['open_shrubland']\n",
    "    grassland = npz['grassland']\n",
    "    barren = npz['barren']\n",
    "    urban = npz['urban']\n",
    "    \n",
    "    all_other = cropland_other + water + evergreen_needleleaf + evergreen_broadleaf + deciduous_needleleaf + deciduous_broadleaf + mixed_forest + woodland + wooded_grassland + closed_shurbland + open_shrubland + grassland + barren + urban\n",
    "    \n",
    "    # Geophysical\n",
    "    elev_std = npz['elev_std']\n",
    "    \n",
    "    lats = npz['lats']\n",
    "    lons = npz['lons']\n",
    "    \n",
    "    # Initial conditions\n",
    "    Ws_init = npz['soilMoist_init']\n",
    "\n",
    "    ##########################\n",
    "    # Prepare inputs for vmap:\n",
    "    # spatial dimensions need to be collapsed and first\n",
    "    # NaN gridpoints need to be removed\n",
    "    nx = tas.shape[0]\n",
    "    ny = tas.shape[1]\n",
    "    nt = tas.shape[2]\n",
    "\n",
    "    assert nt % 365 == 0\n",
    "    nyrs = int(nt / 365)\n",
    "\n",
    "    ## Obs\n",
    "    ys = obs.reshape(nx * ny, nt)\n",
    "    nan_inds_obs = jnp.isnan(ys).any(axis=1)\n",
    "\n",
    "    ## Forcing: all days\n",
    "    tas_in = tas.reshape(nx * ny, nt)\n",
    "    prcp_in = prcp.reshape(nx * ny, nt)\n",
    "\n",
    "    x_forcing_nt = jnp.stack([tas_in, prcp_in], axis=1)\n",
    "    nan_inds_forcing_nt = jnp.isnan(x_forcing_nt).any(axis=(1,2))\n",
    "\n",
    "    ## Forcing: yearly\n",
    "    lai_in = lai.reshape(nx * ny, 365)\n",
    "    x_forcing_nyrs = lai_in\n",
    "    nan_inds_forcing_nyrs = jnp.isnan(x_forcing_nyrs).any(axis=1)\n",
    "\n",
    "    ## Maps\n",
    "    awCap_in = awCap.reshape(nx * ny)\n",
    "    wiltingp_in = wiltingp.reshape(nx * ny)\n",
    "\n",
    "    Ws_init_in = Ws_init.reshape(nx * ny)\n",
    "\n",
    "    clayfrac_in = clayfrac.reshape(nx * ny)\n",
    "    sandfrac_in = sandfrac.reshape(nx * ny)\n",
    "    siltfrac_in = siltfrac.reshape(nx * ny)\n",
    "\n",
    "    lats_in = np.tile(lats, nx)\n",
    "    elev_std_in = elev_std.reshape(nx * ny)\n",
    "\n",
    "    corn_in = corn.reshape(nx * ny)\n",
    "    cotton_in = cotton.reshape(nx * ny)\n",
    "    rice_in = rice.reshape(nx * ny)\n",
    "    sorghum_in = sorghum.reshape(nx * ny)\n",
    "    soybeans_in = soybeans.reshape(nx * ny)\n",
    "    wheat_in = wheat.reshape(nx * ny)\n",
    "\n",
    "    all_other_in = all_other.reshape(nx * ny)\n",
    "\n",
    "    x_maps = jnp.stack([awCap_in, wiltingp_in, \n",
    "                        Ws_init_in, \n",
    "                        clayfrac_in, sandfrac_in, siltfrac_in, \n",
    "                        lats_in, elev_std_in,\n",
    "                        corn_in, cotton_in, rice_in, sorghum_in, soybeans_in, wheat_in],\n",
    "                       axis=1)\n",
    "    nan_inds_maps = jnp.isnan(x_maps).any(axis=1)\n",
    "\n",
    "    # Remove NaNs if desired\n",
    "    if remove_nans:\n",
    "        nan_inds = nan_inds_obs + nan_inds_forcing_nt + nan_inds_forcing_nyrs + nan_inds_maps\n",
    "        ys = ys[~nan_inds]\n",
    "        x_forcing_nt = x_forcing_nt[~nan_inds]\n",
    "        x_forcing_nyrs = x_forcing_nyrs[~nan_inds]\n",
    "        x_maps = x_maps[~nan_inds]\n",
    "\n",
    "    # Return\n",
    "    return ys, x_forcing_nt, x_forcing_nyrs, x_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356af637-5581-4b69-b702-6f2b6aa6cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(theta, constants, x_forcing_nt, x_forcing_nyrs, x_maps):\n",
    "    # Read inputs\n",
    "    tas, prcp = x_forcing_nt\n",
    "    lai = x_forcing_nyrs\n",
    "    \n",
    "    awCap, wiltingp, \\\n",
    "    Ws_init, \\\n",
    "    clayfrac, sandfrac, siltfrac, \\\n",
    "    lats, elev_std, \\\n",
    "    corn, cotton, rice, sorghum, soybeans, wheat \\\n",
    "    = x_maps\n",
    "\n",
    "    # Define all constants\n",
    "    Ts, Tm, Wi_init, Sp_init = constants \n",
    "    \n",
    "    # Define all params\n",
    "    awCap_scalar, wiltingp_scalar, \\\n",
    "    alpha_claycoef, alpha_sandcoef, alpha_siltcoef, \\\n",
    "    betaHBV_claycoef, betaHBV_sandcoef, betaHBV_siltcoef, betaHBV_elevcoef, \\\n",
    "    GS_start_corn, GS_end_corn, L_ini_corn, L_dev_corn, L_mid_corn, Kc_ini_corn, Kc_mid_corn, Kc_end_corn, K_min_corn, K_max_corn, \\\n",
    "    GS_start_cotton, GS_end_cotton, L_ini_cotton, L_dev_cotton, L_mid_cotton, Kc_ini_cotton, Kc_mid_cotton, Kc_end_cotton, K_min_cotton, K_max_cotton, \\\n",
    "    GS_start_rice, GS_end_rice, L_ini_rice, L_dev_rice, L_mid_rice, Kc_ini_rice, Kc_mid_rice, Kc_end_rice, K_min_rice, K_max_rice,  \\\n",
    "    GS_start_sorghum, GS_end_sorghum, L_ini_sorghum, L_dev_sorghum, L_mid_sorghum, Kc_ini_sorghum, Kc_mid_sorghum, Kc_end_sorghum, K_min_sorghum, K_max_sorghum, \\\n",
    "    GS_start_soybeans, GS_end_soybeans, L_ini_soybeans, L_dev_soybeans, L_mid_soybeans, Kc_ini_soybeans, Kc_mid_soybeans, Kc_end_soybeans, K_min_soybeans, K_max_soybeans, \\\n",
    "    GS_start_wheat, GS_end_wheat, L_ini_wheat, L_dev_wheat, L_mid_wheat, Kc_ini_wheat, Kc_mid_wheat, Kc_end_wheat, K_min_wheat, K_max_wheat \\\n",
    "    = jnp.exp(theta)\n",
    "\n",
    "    # Construct Kpet as weighted average\n",
    "    Kpet_corn = construct_Kpet_vec(GS_start_corn, GS_end_corn, L_ini_corn, L_dev_corn, L_mid_corn, 1. - (L_ini_corn + L_dev_corn + L_mid_corn), Kc_ini_corn, Kc_mid_corn, Kc_end_corn, K_min_corn, K_max_corn, lai)\n",
    "    Kpet_cotton = construct_Kpet_vec(GS_start_cotton, GS_end_cotton, L_ini_cotton, L_dev_cotton, L_mid_cotton, 1. - (L_ini_cotton + L_dev_cotton + L_mid_cotton), Kc_ini_cotton, Kc_mid_cotton, Kc_end_cotton, K_min_cotton, K_max_cotton, lai)\n",
    "    Kpet_rice = construct_Kpet_vec(GS_start_rice, GS_end_rice, L_ini_rice, L_dev_rice, L_mid_rice, 1. - (L_ini_rice + L_dev_rice + L_mid_rice), Kc_ini_rice, Kc_mid_rice, Kc_end_rice, K_min_rice, K_max_rice, lai)\n",
    "    Kpet_sorghum = construct_Kpet_vec(GS_start_sorghum, GS_end_sorghum, L_ini_sorghum, L_dev_sorghum, L_mid_sorghum, 1. - (L_ini_sorghum + L_dev_sorghum + L_mid_sorghum), Kc_ini_sorghum, Kc_mid_sorghum, Kc_end_sorghum, K_min_sorghum, K_max_sorghum, lai)\n",
    "    Kpet_soybeans = construct_Kpet_vec(GS_start_soybeans, GS_end_soybeans, L_ini_soybeans, L_dev_soybeans, L_mid_soybeans, 1. - (L_ini_soybeans + L_dev_soybeans + L_mid_soybeans), Kc_ini_soybeans, Kc_mid_soybeans, Kc_end_soybeans, K_min_soybeans, K_max_soybeans, lai)\n",
    "    Kpet_wheat = construct_Kpet_vec(GS_start_wheat, GS_end_wheat, L_ini_wheat, L_dev_wheat, L_mid_wheat, 1. - (L_ini_wheat + L_dev_wheat + L_mid_wheat), Kc_ini_wheat, Kc_mid_wheat, Kc_end_wheat, K_min_wheat, K_max_wheat, lai)\n",
    "\n",
    "    other = 1. - (corn + cotton + rice + sorghum + soybeans + wheat)\n",
    "    weights = jnp.array([corn, cotton, rice, sorghum, soybeans, wheat, other])\n",
    "    Kpets = jnp.array([Kpet_corn, Kpet_cotton, Kpet_rice, Kpet_sorghum, Kpet_soybeans, Kpet_wheat, jnp.ones(365)])\n",
    "    Kpet = jnp.average(Kpets, weights = weights, axis=0)\n",
    "    \n",
    "    # params that WBM sees\n",
    "    awCap_scaled = awCap * awCap_scalar\n",
    "    wiltingp_scaled = wiltingp * wiltingp_scalar\n",
    "    alpha = 1.0 + (alpha_claycoef * clayfrac) + (alpha_sandcoef * sandfrac) + (alpha_siltcoef * siltfrac)\n",
    "    betaHBV = 1.0 + (betaHBV_claycoef * clayfrac) + (betaHBV_sandcoef * sandfrac) + (betaHBV_siltcoef * siltfrac) + (betaHBV_elevcoef * elev_std)\n",
    "    \n",
    "    params = (Ts, Tm, wiltingp_scaled, awCap_scaled, alpha, betaHBV)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = wbm_jax(\n",
    "        tas,\n",
    "        prcp, \n",
    "        Kpet,\n",
    "        Ws_init,\n",
    "        Wi_init,\n",
    "        Sp_init,\n",
    "        lai,\n",
    "        lats,\n",
    "        params\n",
    "    )\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90258cb5-f985-468b-8ca7-89e434f3f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_store(subset_name, obs_name, _error_fn, error_fn_name, n_epochs,\n",
    "                    batch_size = 2**7,\n",
    "                    opt = 'adam',\n",
    "                    learning_rate = 1e-2,\n",
    "                    val_frac = 0.2,\n",
    "                    reg_const = 0.01,\n",
    "                    initial_params = initial_params,\n",
    "                    params_lower = params_lower,\n",
    "                    params_upper = params_upper):\n",
    "    #############################################\n",
    "    # Loss function with correct error metric\n",
    "    ############################################\n",
    "    # Prediction loss\n",
    "    def prediction_loss(theta, constants,\n",
    "                        x_forcing_nt, x_forcing_nyrs, x_maps, ys):\n",
    "        \n",
    "        prediction = make_prediction(theta, constants, x_forcing_nt, x_forcing_nyrs, x_maps)\n",
    "        \n",
    "        return _error_fn(prediction, ys)\n",
    "    \n",
    "    # Regularization loss\n",
    "    def reg_loss(theta, initial_params, params_lower, params_upper):\n",
    "        \n",
    "        return jnp.nansum((theta - initial_params)**2 / ((theta - params_lower) * (params_upper - theta)))\n",
    "    \n",
    "    # Total loss\n",
    "    def loss_fn(theta, reg_const, initial_params, params_lower, params_upper, constants,\n",
    "                x_forcing_nt, x_forcing_nyrs, x_maps, ys):\n",
    "        \n",
    "        return prediction_loss(theta, constants, x_forcing_nt, x_forcing_nyrs, x_maps, ys) + \\\n",
    "                reg_const * reg_loss(theta, initial_params, params_lower, params_upper)\n",
    "    \n",
    "    # jit and vmap it\n",
    "    pred_loss_value = jax.jit(jax.vmap(prediction_loss, in_axes=(None, None, 0, 0, 0, 0), out_axes=0))\n",
    "    loss_value_and_grad = jax.jit(jax.vmap(jax.value_and_grad(loss_fn), in_axes=(None, None, None, None, None, None, 0, 0, 0, 0), out_axes=0))\n",
    "\n",
    "    ###########################\n",
    "    # Setup\n",
    "    ###########################\n",
    "    # Read data\n",
    "    ys, x_forcing_nt, x_forcing_nyrs, x_maps = read_inputs(subset_name, obs_name, True)\n",
    "    N = ys.shape[0]\n",
    "    \n",
    "    # Get train/val split over space\n",
    "    N_val = int(N * val_frac)\n",
    "    N_train = N - N_val\n",
    "    \n",
    "    train_idx = np.random.choice(N, N_train, replace=False)\n",
    "    ys_train, x_forcing_nt_train, x_forcing_nyrs_train, x_maps_train = ys[train_idx], x_forcing_nt[train_idx], x_forcing_nyrs[train_idx], x_maps[train_idx]\n",
    "\n",
    "    if N_val > 0:\n",
    "        val_idx = np.array([n for n in np.arange(N) if n not in train_idx])\n",
    "        ys_val, x_forcing_nt_val, x_forcing_nyrs_val, x_maps_val = ys[val_idx], x_forcing_nt[val_idx], x_forcing_nyrs[val_idx], x_maps[val_idx]\n",
    "    \n",
    "    # Define mini-batch hyper-parameters\n",
    "    # n_minibatches = 1 + N // batch_size\n",
    "    n_minibatches = 1 + N_train // batch_size\n",
    "\n",
    "    # Initial parameters\n",
    "    # theta = np.random.uniform(low=params_lower, high=params_upper)\n",
    "    # theta = np.random.normal(loc=initial_params, scale=abs(initial_params/10.))\n",
    "    theta = initial_params\n",
    "\n",
    "    # Optimizer\n",
    "    if opt == 'adam':\n",
    "        adam = optax.adam(learning_rate=learning_rate)\n",
    "        opt_fn = adam.update\n",
    "        opt_state = adam.init(theta)\n",
    "    elif opt == 'sgd':\n",
    "        learning_rate = 1e-5\n",
    "        opt_state = None\n",
    "        def sgd(gradients, state):\n",
    "            return -learning_rate * gradients, state\n",
    "        opt_fn = sgd\n",
    "\n",
    "    # Loss\n",
    "    train_loss_out = np.empty(n_epochs + 1)\n",
    "    pred_loss_out = np.empty(n_epochs + 1)\n",
    "    reg_loss_out = np.empty(n_epochs + 1)\n",
    "    val_loss_out = np.empty(n_epochs + 1)\n",
    "\n",
    "    # Where to store results\n",
    "    datetime_str = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "    training_name = f\"{error_fn_name}_{str(n_epochs)}epochs_{str(batch_size)}batchsize_{opt}-opt_{str(val_frac)}val_{str(reg_const)}reg_{datetime_str}\"\n",
    "\n",
    "    out_file_path = f\"{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/training_res/{training_name}.txt\"\n",
    "    f = open(out_file_path, \"w\")\n",
    "    f.write(f\"epoch metric train_loss pred_loss reg_loss val_loss {' '.join(param_names)}\\n\")\n",
    "\n",
    "    # initial results\n",
    "    pred_loss_init = jnp.mean(pred_loss_value(theta,\n",
    "                                              constants,\n",
    "                                              x_forcing_nt_train,\n",
    "                                              x_forcing_nyrs_train,\n",
    "                                              x_maps_train,\n",
    "                                              ys_train))\n",
    "    if N_val > 0:\n",
    "        val_loss_init = jnp.mean(pred_loss_value(theta,\n",
    "                                                 constants,\n",
    "                                                 x_forcing_nt_val,\n",
    "                                                 x_forcing_nyrs_val,\n",
    "                                                 x_maps_val,\n",
    "                                                 ys_val))\n",
    "    else:\n",
    "        val_loss_init = np.nan\n",
    "        \n",
    "    reg_loss_init = reg_loss(theta, initial_params, params_lower, params_upper)\n",
    "    print(f\"Epoch 0 pred loss: {pred_loss_init:.4f}, reg_loss: {reg_loss_init:.4f}, val loss: {val_loss_init:.4f}\")\n",
    "    \n",
    "    ###########################\n",
    "    # Training loop\n",
    "    ###########################\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        # Shuffle indices\n",
    "        shuffled_inds = np.random.permutation(N_train)\n",
    "    \n",
    "        # Generate a mini-batch\n",
    "        minibatch_inds = [shuffled_inds[(i*batch_size):((i + 1)*batch_size)] for i in range(n_minibatches)]\n",
    "\n",
    "        # For batch loss\n",
    "        batch_loss = [None] * n_minibatches\n",
    "\n",
    "        for idx, inds in enumerate(minibatch_inds):\n",
    "            # Calculate gradient of loss function, update parameters\n",
    "            loss, grads = loss_value_and_grad(theta, reg_const, initial_params, params_lower, params_upper, constants,\n",
    "                                              x_forcing_nt_train[inds],\n",
    "                                              x_forcing_nyrs_train[inds],\n",
    "                                              x_maps_train[inds],\n",
    "                                              ys_train[inds])\n",
    "            updates, opt_state = opt_fn(jnp.nanmean(grads, axis=0), opt_state)\n",
    "            theta = optax.apply_updates(theta, updates)\n",
    "            batch_loss[idx] = loss\n",
    "            # Break if theta steps outside bounds\n",
    "            if (theta < params_lower).any() or (theta > params_upper).any():\n",
    "                print('Found invalid parameter')\n",
    "                f.close()\n",
    "                os.remove(out_file_path)\n",
    "                return None\n",
    "\n",
    "        # Save all losses\n",
    "        train_loss_out[epoch] = jnp.nanmean(jnp.array([item for row in batch_loss for item in row]))\n",
    "        reg_loss_out[epoch] = reg_loss(theta, initial_params, params_lower, params_upper)\n",
    "        pred_loss_out[epoch] = train_loss_out[epoch] - (reg_const * reg_loss_out[epoch])\n",
    "        if N_val > 0:\n",
    "            val_loss_out[epoch] = jnp.mean(pred_loss_value(theta,\n",
    "                                                          constants,\n",
    "                                                          x_forcing_nt_val,\n",
    "                                                          x_forcing_nyrs_val,\n",
    "                                                          x_maps_val,\n",
    "                                                          ys_val))\n",
    "        else:\n",
    "            val_loss_out[epoch] = jnp.nan\n",
    "        \n",
    "        # Write every epoch\n",
    "        theta_str = [str(param) for param in theta]\n",
    "        f.write(f\"{str(epoch + 1)} {error_fn_name} {train_loss_out[epoch]:.4f} {pred_loss_out[epoch]:.4f} {reg_loss_out[epoch]:.4f} {val_loss_out[epoch]:.4f} {' '.join(theta_str)}\\n\")\n",
    "        # Print every 5\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {str(epoch + 1)} total loss: {train_loss_out[epoch]:.4f}, pred loss: {pred_loss_out[epoch]:.4f}, reg_loss: {reg_loss_out[epoch]:.4f}, val loss: {val_loss_out[epoch]:.4f}\")\n",
    "\n",
    "    f.close()\n",
    "    return train_loss_out, pred_loss_out, reg_loss_out, val_loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7849bfd0-5f4e-4402-8408-9fbc4cce6f0a",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbfca15-2fec-4ca2-883d-75d8efc90cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of timeseries needed for quantile RMSE\n",
    "N = 2555\n",
    "\n",
    "# Define all error functions\n",
    "# RMSE\n",
    "_rmse = lambda prediction, ys: jnp.sqrt(jnp.nanmean((prediction-ys)**2))\n",
    "\n",
    "# MSE\n",
    "_mse = lambda prediction, ys: jnp.nanmean((prediction-ys)**2)\n",
    "\n",
    "# KGE\n",
    "def _kge(prediction, ys):\n",
    "    corr = jnp.nanmean((prediction - jnp.nanmean(prediction))*(ys - jnp.nanmean(ys))) / (jnp.nanstd(prediction) * jnp.nanstd(ys))\n",
    "    mean_ratio = jnp.nanmean(prediction) / jnp.nanmean(ys)\n",
    "    std_ratio = jnp.nanstd(prediction) / jnp.nanstd(ys)\n",
    "    kge = 1 - jnp.sqrt((corr - 1)**2 + (mean_ratio - 1)**2 + (std_ratio - 1)**2)\n",
    "    return -kge \n",
    "\n",
    "# q0-25 RMSE\n",
    "qmax = 0.25\n",
    "size = round(N * qmax)\n",
    "def _q25rmse(prediction, ys):\n",
    "    thresh = jnp.quantile(ys, qmax)\n",
    "    inds = jnp.where(ys <= thresh, size=size)\n",
    "    prediction_q = prediction[inds]\n",
    "    ys_q = ys[inds]\n",
    "    return jnp.sqrt(jnp.nanmean((prediction_q - ys_q)**2))\n",
    "\n",
    "# q75-100 RMSE\n",
    "qmin = 0.75\n",
    "size = round(N * qmin)\n",
    "def _q75rmse(prediction, ys):\n",
    "    thresh = jnp.quantile(ys, qmin)\n",
    "    inds = jnp.where(ys >= thresh, size=size)\n",
    "    prediction_q = prediction[inds]\n",
    "    ys_q = ys[inds]\n",
    "    return jnp.sqrt(jnp.nanmean((prediction_q - ys_q)**2))\n",
    "\n",
    "_error_fns = [_rmse, _mse, _kge, _q25rmse, _q75rmse]\n",
    "error_fn_names = ['rmse', 'mse', 'kge', 'q0-25rmse', 'q75-100rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746119c-d74f-4f54-a3d3-62809522617a",
   "metadata": {},
   "source": [
    "### SMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66c3bf2-ed26-4d19-bdde-66530af5c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "subset_name = 'centralUS'\n",
    "obs_name = 'SMAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ccef39-b939-40ad-b694-0cd667d7a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parallelize with dask delayed\n",
    "delayed = []\n",
    "\n",
    "for _error_fn, error_fn_name in zip(_error_fns, error_fn_names):\n",
    "    # Hyperparameter adjustments\n",
    "    if error_fn_name == 'kge':\n",
    "        reg_const = 0.001\n",
    "    else:\n",
    "        reg_const = 0.01\n",
    "\n",
    "    if error_fn_name == 'mse':\n",
    "        learning_rate = 1e-3\n",
    "    else:\n",
    "        learning_rate = 1e-2\n",
    "        \n",
    "    for batch_size in [2**5, 2**6, 2**7, 2**8, 2**9]:\n",
    "        delayed.append(dask.delayed(train_and_store)(subset_name = subset_name,\n",
    "                                                     obs_name = obs_name,\n",
    "                                                     _error_fn = _error_fn,\n",
    "                                                     error_fn_name = error_fn_name,\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     reg_const = reg_const,\n",
    "                                                     learning_rate = learning_rate,\n",
    "                                                     n_epochs = 30,\n",
    "                                                     val_frac = 0.0))\n",
    "\n",
    "out = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78298f-5adc-4abe-9c51-0e8d4699cc6e",
   "metadata": {},
   "source": [
    "### VIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b521f9df-98f6-48a2-aa33-a8c015979e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "subset_name = 'centralUS'\n",
    "obs_name = 'VIC'\n",
    "\n",
    "# needed for quantile RMSE\n",
    "N = np.load(f'{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/{obs_name}_validation.npy').shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c10b78-78ac-4912-b5f7-daba0f967211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 41s, sys: 21.2 s, total: 5min 2s\n",
      "Wall time: 56min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parallelize with dask delayed\n",
    "delayed = []\n",
    "\n",
    "for _error_fn, error_fn_name in zip(_error_fns, error_fn_names):\n",
    "    # Hyperparameter adjustments\n",
    "    if error_fn_name == 'kge':\n",
    "        reg_const = 0.001\n",
    "    else:\n",
    "        reg_const = 0.01\n",
    "\n",
    "    if error_fn_name == 'mse':\n",
    "        learning_rate = 1e-3\n",
    "    else:\n",
    "        learning_rate = 1e-2\n",
    "        \n",
    "    for batch_size in [2**5, 2**6, 2**7, 2**8, 2**9]:\n",
    "        delayed.append(dask.delayed(train_and_store)(subset_name = subset_name,\n",
    "                                                     obs_name = obs_name,\n",
    "                                                     _error_fn = _error_fn,\n",
    "                                                     error_fn_name = error_fn_name,\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     reg_const = reg_const,\n",
    "                                                     learning_rate = learning_rate,\n",
    "                                                     n_epochs = 30,\n",
    "                                                     val_frac = 0.0))\n",
    "\n",
    "out = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f30aa-6bdc-491c-9efb-4adcf412fcde",
   "metadata": {},
   "source": [
    "### MOSAIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34239120-98d7-44a9-be56-2061e53f8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "subset_name = 'centralUS'\n",
    "obs_name = 'MOSAIC'\n",
    "\n",
    "# needed for quantile RMSE\n",
    "N = np.load(f'{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/{obs_name}_validation.npy').shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f882d851-9169-4706-b866-fb52b3c33c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 23s, sys: 20.8 s, total: 4min 43s\n",
      "Wall time: 56min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parallelize with dask delayed\n",
    "delayed = []\n",
    "\n",
    "for _error_fn, error_fn_name in zip(_error_fns, error_fn_names):\n",
    "    # Hyperparameter adjustments\n",
    "    if error_fn_name == 'kge':\n",
    "        reg_const = 0.001\n",
    "    else:\n",
    "        reg_const = 0.01\n",
    "\n",
    "    if error_fn_name == 'mse':\n",
    "        learning_rate = 1e-3\n",
    "    else:\n",
    "        learning_rate = 1e-2\n",
    "        \n",
    "    for batch_size in [2**5, 2**6, 2**7, 2**8, 2**9]:\n",
    "        delayed.append(dask.delayed(train_and_store)(subset_name = subset_name,\n",
    "                                                     obs_name = obs_name,\n",
    "                                                     _error_fn = _error_fn,\n",
    "                                                     error_fn_name = error_fn_name,\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     reg_const = reg_const,\n",
    "                                                     learning_rate = learning_rate,\n",
    "                                                     n_epochs = 30,\n",
    "                                                     val_frac = 0.0))\n",
    "\n",
    "out = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dac970-d6d5-454a-a595-7c7d44487e61",
   "metadata": {},
   "source": [
    "### NOAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a960759-af42-4edb-8066-ad431f90c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "subset_name = 'centralUS'\n",
    "obs_name = 'NOAH'\n",
    "\n",
    "# needed for quantile RMSE\n",
    "N = np.load(f'{project_data_path}/WBM/calibration/{subset_name}/{obs_name}/{obs_name}_validation.npy').shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e9352-6283-411a-97ec-ce4866781bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parallelize with dask delayed\n",
    "delayed = []\n",
    "\n",
    "for _error_fn, error_fn_name in zip(_error_fns, error_fn_names):\n",
    "    # Hyperparameter adjustments\n",
    "    if error_fn_name == 'kge':\n",
    "        reg_const = 0.001\n",
    "    else:\n",
    "        reg_const = 0.01\n",
    "\n",
    "    if error_fn_name == 'mse':\n",
    "        learning_rate = 1e-3\n",
    "    else:\n",
    "        learning_rate = 1e-2\n",
    "        \n",
    "    for batch_size in [2**5, 2**6, 2**7, 2**8, 2**9]:\n",
    "        delayed.append(dask.delayed(train_and_store)(subset_name = subset_name,\n",
    "                                                     obs_name = obs_name,\n",
    "                                                     _error_fn = _error_fn,\n",
    "                                                     error_fn_name = error_fn_name,\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     reg_const = reg_const,\n",
    "                                                     learning_rate = learning_rate,\n",
    "                                                     n_epochs = 30,\n",
    "                                                     val_frac = 0.0))\n",
    "\n",
    "out = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872df4b-4415-4b2d-a666-2b98dd230e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137beaed-9ad8-4da4-bade-d02819c31ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
