{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a579e8-591e-4400-adbc-380f6a233996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.stats import qmc\n",
    "import regionmask\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a825a7db-a80a-4071-bce0-5d2036006a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Directories ##\n",
    "#################\n",
    "nldas_path = '/storage/group/pches/default/public/NLDAS/'\n",
    "smap_path = '/storage/group/pches/default/public/SMAP/'\n",
    "project_data_path = '/storage/group/pches/default/users/dcl5300/wbm_soilM_crop_uc_lafferty-etal-2024-tbd_DATA'\n",
    "log_path = '/storage/home/dcl5300/work/current_projects/wbm_soilM_crop_uc_lafferty-etal-2024-tbd/code/logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3e375-1516-418d-8136-5e281caecd87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f789d544-cd3c-4297-98ae-249247269417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    account=\"pches\",\n",
    "    # account=\"open\",\n",
    "    cores=1,\n",
    "    memory=\"5GiB\",\n",
    "    walltime=\"02:00:00\",\n",
    ")\n",
    "\n",
    "cluster.scale(jobs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161bbcd8-92bd-4781-923d-3e5219b57d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f993cd63-b2cd-4007-b433-47208364e029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-52df6371-9377-11ee-b68c-0050569d9608</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.SLURMCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">SLURMCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">50f0b032</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-0cdec53c-ab7e-41de-be16-085ecfa19b53</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://146.186.150.11:46615\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://146.186.150.11:46615' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1741a-a9e8-4d75-b26f-6ab787ebfbe6",
   "metadata": {},
   "source": [
    "# Precalibration (central US test case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe095f0-5303-4180-863c-fd8569e6ac07",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e84524-da44-40e6-b4ba-df8cc385aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to central US\n",
    "def _subset_centralUS(ds):\n",
    "    # Define central US\n",
    "    centralUS_states = [\"Illinois\", \"Iowa\", \"Wisconsin\", \"Minnesota\", \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\", \"Missouri\", \"Indiana\", \"Ohio\", \"Michigan\", \"Kentucky\"]\n",
    "    # Subset\n",
    "    centralUS_index = regionmask.defined_regions.natural_earth_v5_0_0.us_states_50.map_keys(centralUS_states)\n",
    "    centralUS_mask = regionmask.defined_regions.natural_earth_v5_0_0.us_states_50.mask(ds)\n",
    "    ds_centralUS = ds.where(centralUS_mask.isin(centralUS_index), drop=True)\n",
    "    # Return\n",
    "    return ds_centralUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616f395f-1571-45dd-b39c-c192f44fe24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### Uncertain parameters: append range ###\n",
    "##########################################\n",
    "n_sample = 5_000\n",
    "\n",
    "if True:\n",
    "    ###  Define bounds\n",
    "    # awCap factor range ~ 0.1 - 0.5\n",
    "    awCap_factor_lower, awCap_factor_upper = 0.1, 0.5\n",
    "\n",
    "    # wiltingp factor range ~ 0.1 - 0.5\n",
    "    wiltingp_factor_lower, wiltingp_factor_upper = 0.1, 0.5\n",
    "\n",
    "    # alpha range ~ 1 - 10\n",
    "    alpha_claycoef_lower, alpha_claycoef_upper = 0.0, 10.\n",
    "    alpha_sandcoef_lower, alpha_sandcoef_upper = 0.0, 10.\n",
    "    alpha_siltcoef_lower, alpha_siltcoef_upper = 0.0, 10.\n",
    "\n",
    "    # betaHBV range ~ 1 - 10\n",
    "    betaHBV_claycoef_lower, betaHBV_claycoef_upper = 0.0, 10.\n",
    "    betaHBV_sandcoef_lower, betaHBV_sandcoef_upper = 0.0, 10.\n",
    "    betaHBV_siltcoef_lower, betaHBV_siltcoef_upper = 0.0, 10.\n",
    "\n",
    "    # GS_start_lower, GS_start_upper = 50, 200\n",
    "    # GS_length_lower, GS_length_upper = 100, 200\n",
    "    # rootDepth_oGS_lower, rootDepth_oGS_upper = 100, 1200\n",
    "    # rootDepth_GS_factor_lower, rootDepth_GS_factor_upper = 2., 10.\n",
    "    # rootDepth_all_lower, rootDepth_all_upper = 100, 2500\n",
    "\n",
    "    # LHC sampling\n",
    "    sampler = qmc.LatinHypercube(d=8)\n",
    "    sample = sampler.random(n=n_sample)\n",
    "\n",
    "    # l_bounds = [fieldCap_claycoef_lower, fieldCap_sandcoef_lower, fieldCap_siltcoef_lower,\n",
    "    #             wiltingp_claycoef_lower, wiltingp_sandcoef_lower, wiltingp_siltcoef_lower,\n",
    "    #             alpha_claycoef_lower, alpha_sandcoef_lower, alpha_siltcoef_lower,\n",
    "    #             betaHBV_claycoef_lower, betaHBV_sandcoef_lower, betaHBV_siltcoef_lower]\n",
    "    \n",
    "    # u_bounds = [fieldCap_claycoef_upper, fieldCap_sandcoef_upper, fieldCap_siltcoef_upper,\n",
    "    #             wiltingp_claycoef_upper, wiltingp_sandcoef_upper, wiltingp_siltcoef_upper,\n",
    "    #             alpha_claycoef_upper, alpha_sandcoef_upper, alpha_siltcoef_upper,\n",
    "    #             betaHBV_claycoef_upper, betaHBV_sandcoef_upper, betaHBV_siltcoef_upper]\n",
    "\n",
    "    l_bounds = [awCap_factor_lower,\n",
    "                wiltingp_factor_lower,\n",
    "                alpha_claycoef_lower, alpha_sandcoef_lower, alpha_siltcoef_lower,\n",
    "                betaHBV_claycoef_lower, betaHBV_sandcoef_lower, betaHBV_siltcoef_lower]\n",
    "    \n",
    "    u_bounds = [awCap_factor_upper,\n",
    "                wiltingp_factor_upper,\n",
    "                alpha_claycoef_upper, alpha_sandcoef_upper, alpha_siltcoef_upper,\n",
    "                betaHBV_claycoef_upper, betaHBV_sandcoef_upper, betaHBV_siltcoef_upper]\n",
    "\n",
    "    sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "    # Store as csv\n",
    "    df = pd.DataFrame(data = sample_scaled,\n",
    "                      columns = ['awCap_factor', 'wiltingp_factor',\n",
    "                                 'alpha_claycoef', 'alpha_sandcoef', 'alpha_siltcoef',\n",
    "                                 'betaHBV_claycoef', 'betaHBV_sandcoef', 'betaHBV_siltcoef'])\n",
    "    df['iparam'] = df.index + 50_000\n",
    "    df.to_csv(f'{project_data_path}/WBM/precalibration/centralUS/params_extra_low.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b6cadb-4d0b-4cf4-9bb3-12d60fce4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### Uncertain parameters: append range ###\n",
    "##########################################\n",
    "n_sample = 5_000\n",
    "\n",
    "if True:\n",
    "    ###  Define bounds\n",
    "    # awCap factor range ~ 2 - 5\n",
    "    awCap_factor_lower, awCap_factor_upper = 2, 5\n",
    "\n",
    "    # wiltingp factor range ~ 2 - 5\n",
    "    wiltingp_factor_lower, wiltingp_factor_upper = 2, 5\n",
    "\n",
    "    # alpha range ~ 1 - 10\n",
    "    alpha_claycoef_lower, alpha_claycoef_upper = 0.0, 10.\n",
    "    alpha_sandcoef_lower, alpha_sandcoef_upper = 0.0, 10.\n",
    "    alpha_siltcoef_lower, alpha_siltcoef_upper = 0.0, 10.\n",
    "\n",
    "    # betaHBV range ~ 1 - 10\n",
    "    betaHBV_claycoef_lower, betaHBV_claycoef_upper = 0.0, 10.\n",
    "    betaHBV_sandcoef_lower, betaHBV_sandcoef_upper = 0.0, 10.\n",
    "    betaHBV_siltcoef_lower, betaHBV_siltcoef_upper = 0.0, 10.\n",
    "\n",
    "    # GS_start_lower, GS_start_upper = 50, 200\n",
    "    # GS_length_lower, GS_length_upper = 100, 200\n",
    "    # rootDepth_oGS_lower, rootDepth_oGS_upper = 100, 1200\n",
    "    # rootDepth_GS_factor_lower, rootDepth_GS_factor_upper = 2., 10.\n",
    "    # rootDepth_all_lower, rootDepth_all_upper = 100, 2500\n",
    "\n",
    "    # LHC sampling\n",
    "    sampler = qmc.LatinHypercube(d=8)\n",
    "    sample = sampler.random(n=n_sample)\n",
    "\n",
    "    # l_bounds = [fieldCap_claycoef_lower, fieldCap_sandcoef_lower, fieldCap_siltcoef_lower,\n",
    "    #             wiltingp_claycoef_lower, wiltingp_sandcoef_lower, wiltingp_siltcoef_lower,\n",
    "    #             alpha_claycoef_lower, alpha_sandcoef_lower, alpha_siltcoef_lower,\n",
    "    #             betaHBV_claycoef_lower, betaHBV_sandcoef_lower, betaHBV_siltcoef_lower]\n",
    "    \n",
    "    # u_bounds = [fieldCap_claycoef_upper, fieldCap_sandcoef_upper, fieldCap_siltcoef_upper,\n",
    "    #             wiltingp_claycoef_upper, wiltingp_sandcoef_upper, wiltingp_siltcoef_upper,\n",
    "    #             alpha_claycoef_upper, alpha_sandcoef_upper, alpha_siltcoef_upper,\n",
    "    #             betaHBV_claycoef_upper, betaHBV_sandcoef_upper, betaHBV_siltcoef_upper]\n",
    "\n",
    "    l_bounds = [awCap_factor_lower,\n",
    "                wiltingp_factor_lower,\n",
    "                alpha_claycoef_lower, alpha_sandcoef_lower, alpha_siltcoef_lower,\n",
    "                betaHBV_claycoef_lower, betaHBV_sandcoef_lower, betaHBV_siltcoef_lower]\n",
    "    \n",
    "    u_bounds = [awCap_factor_upper,\n",
    "                wiltingp_factor_upper,\n",
    "                alpha_claycoef_upper, alpha_sandcoef_upper, alpha_siltcoef_upper,\n",
    "                betaHBV_claycoef_upper, betaHBV_sandcoef_upper, betaHBV_siltcoef_upper]\n",
    "\n",
    "    sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "    # Store as csv\n",
    "    df = pd.DataFrame(data = sample_scaled,\n",
    "                      columns = ['awCap_factor', 'wiltingp_factor',\n",
    "                                 'alpha_claycoef', 'alpha_sandcoef', 'alpha_siltcoef',\n",
    "                                 'betaHBV_claycoef', 'betaHBV_sandcoef', 'betaHBV_siltcoef'])\n",
    "    df['iparam'] = df.index + 55_000\n",
    "    df.to_csv(f'{project_data_path}/WBM/precalibration/centralUS/params_extra_high.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e90aecb7-a841-47d4-a9b0-655ce6b1724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "df_main = pd.read_csv(f'{project_data_path}/WBM/precalibration/centralUS/params.csv')\n",
    "df_low = pd.read_csv(f'{project_data_path}/WBM/precalibration/centralUS/params_extra_low.csv')\n",
    "df_high = pd.read_csv(f'{project_data_path}/WBM/precalibration/centralUS/params_extra_high.csv')\n",
    "\n",
    "df = pd.concat([df_main, df_low, df_high])\n",
    "df.to_csv(f'{project_data_path}/WBM/precalibration/centralUS/params_extra.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4e249f-0ee1-42fc-9d4e-c08e3d7cc6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fd3499-6bb9-4825-a080-e532ed2eb251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['iparam'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6186ac-5279-4b47-ac91-9d5a33888700",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### Uncertain parameters ###\n",
    "############################\n",
    "n_sample = 50_000\n",
    "\n",
    "if not os.path.isfile(f'{project_data_path}/WBM/precalibration/centralUS/params.csv'):\n",
    "    ###  Define bounds\n",
    "    # # fieldCap range ~ 0 - 0.75 (in m3/m3 -> multiply by 1000 to get mm/m)\n",
    "    # fieldCap_claycoef_lower, fieldCap_claycoef_upper = 0.001, 0.75\n",
    "    # fieldCap_sandcoef_lower, fieldCap_sandcoef_upper = 0.001, 0.75\n",
    "    # fieldCap_siltcoef_lower, fieldCap_siltcoef_upper = 0.001, 0.75\n",
    "\n",
    "    # # wilingp range ~ 0 - 1 (as factor of field capacity)\n",
    "    # wiltingp_claycoef_lower, wiltingp_claycoef_upper = 0.001, 1.\n",
    "    # wiltingp_sandcoef_lower, wiltingp_sandcoef_upper = 0.001, 1.\n",
    "    # wiltingp_siltcoef_lower, wiltingp_siltcoef_upper = 0.001, 1.\n",
    "\n",
    "    # awCap factor range ~ 0.5 - 2\n",
    "    awCap_factor_lower, awCap_factor_upper = 0.5, 2\n",
    "\n",
    "    # wiltingp factor range ~ 0.5 - 2\n",
    "    wiltingp_factor_lower, wiltingp_factor_upper = 0.5, 2\n",
    "\n",
    "    # alpha range ~ 1 - 10\n",
    "    alpha_claycoef_lower, alpha_claycoef_upper = 0.0, 10.\n",
    "    alpha_sandcoef_lower, alpha_sandcoef_upper = 0.0, 10.\n",
    "    alpha_siltcoef_lower, alpha_siltcoef_upper = 0.0, 10.\n",
    "\n",
    "    # betaHBV range ~ 1 - 10\n",
    "    betaHBV_claycoef_lower, betaHBV_claycoef_upper = 0.0, 10.\n",
    "    betaHBV_sandcoef_lower, betaHBV_sandcoef_upper = 0.0, 10.\n",
    "    betaHBV_siltcoef_lower, betaHBV_siltcoef_upper = 0.0, 10.\n",
    "\n",
    "    # GS_start_lower, GS_start_upper = 50, 200\n",
    "    # GS_length_lower, GS_length_upper = 100, 200\n",
    "    # rootDepth_oGS_lower, rootDepth_oGS_upper = 100, 1200\n",
    "    # rootDepth_GS_factor_lower, rootDepth_GS_factor_upper = 2., 10.\n",
    "    # rootDepth_all_lower, rootDepth_all_upper = 100, 2500\n",
    "\n",
    "    # LHC sampling\n",
    "    sampler = qmc.LatinHypercube(d=8)\n",
    "    sample = sampler.random(n=n_sample)\n",
    "\n",
    "    # l_bounds = [fieldCap_claycoef_lower, fieldCap_sandcoef_lower, fieldCap_siltcoef_lower,\n",
    "    #             wiltingp_claycoef_lower, wiltingp_sandcoef_lower, wiltingp_siltcoef_lower,\n",
    "    #             alpha_claycoef_lower, alpha_sandcoef_lower, alpha_siltcoef_lower,\n",
    "    #             betaHBV_claycoef_lower, betaHBV_sandcoef_lower, betaHBV_siltcoef_lower]\n",
    "    \n",
    "    # u_bounds = [fieldCap_claycoef_upper, fieldCap_sandcoef_upper, fieldCap_siltcoef_upper,\n",
    "    #             wiltingp_claycoef_upper, wiltingp_sandcoef_upper, wiltingp_siltcoef_upper,\n",
    "    #             alpha_claycoef_upper, alpha_sandcoef_upper, alpha_siltcoef_upper,\n",
    "    #             betaHBV_claycoef_upper, betaHBV_sandcoef_upper, betaHBV_siltcoef_upper]\n",
    "\n",
    "    l_bounds = [awCap_factor_lower,\n",
    "                wiltingp_factor_lower,\n",
    "                alpha_claycoef_lower, alpha_sandcoef_lower, alpha_siltcoef_lower,\n",
    "                betaHBV_claycoef_lower, betaHBV_sandcoef_lower, betaHBV_siltcoef_lower]\n",
    "    \n",
    "    u_bounds = [awCap_factor_upper,\n",
    "                wiltingp_factor_upper,\n",
    "                alpha_claycoef_upper, alpha_sandcoef_upper, alpha_siltcoef_upper,\n",
    "                betaHBV_claycoef_upper, betaHBV_sandcoef_upper, betaHBV_siltcoef_upper]\n",
    "\n",
    "    sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "    # Store as csv\n",
    "    df = pd.DataFrame(data = sample_scaled,\n",
    "                      columns = ['awCap_factor', 'wiltingp_factor',\n",
    "                                 'alpha_claycoef', 'alpha_sandcoef', 'alpha_siltcoef',\n",
    "                                 'betaHBV_claycoef', 'betaHBV_sandcoef', 'betaHBV_siltcoef'])\n",
    "    df['iparam'] = df.index\n",
    "    df.to_csv(f'{project_data_path}/WBM/precalibration/centralUS/params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e8936e-e02b-400c-aff2-2ece6784ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.pairplot(df.iloc[::100, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba7de59-6e5a-425b-83f9-63cd394dd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "########## SMAP and NLDAS data for validation ##########\n",
    "########################################################\n",
    "########################## SMAP\n",
    "if not os.path.isfile(f'{project_data_path}/WBM/precalibration/centralUS/SMAP/SMAP_validation.nc'):\n",
    "    # Read all\n",
    "    files = glob(f'{smap_path}/processed_nldas_grid/SMAP_L4_SM_gph_all_nldas_*.nc')\n",
    "    ds_smap = xr.concat([_subset_centralUS(xr.open_dataset(file)['sm_rootzone']) for file in files], dim='time')\n",
    "    \n",
    "    # Merge and store (and change units to kg/m3)\n",
    "    ds_out = xr.Dataset({'soilMoist':1000*ds_smap})\n",
    "    ds_out.attrs['units'] = 'kg/m3'\n",
    "    ds_out.to_netcdf(f'{project_data_path}/WBM/precalibration/centralUS/SMAP_validation.nc')\n",
    "\n",
    "    # Also store numpy array for quicker evaluations\n",
    "    npy_out = np.transpose(ds_out['soilMoist'].to_numpy(), (2,1,0))\n",
    "    np.save(f'{project_data_path}/WBM/precalibration/centralUS/SMAP_validation.npy', npy_out)\n",
    "\n",
    "######################### NLDAS\n",
    "nldas_dict = {'VIC':'SOILM0_100cm', 'NOAH':'SOILM', 'MOSAIC':'SOILM'}\n",
    "\n",
    "for model, var_id in nldas_dict.items():\n",
    "    if not os.path.isfile(f'{project_data_path}/WBM/precalibration/centralUS/{model}/{model}_validation.nc'):\n",
    "        # Read all\n",
    "        files = glob(f'{nldas_path}/{model}/daily/*.nc')\n",
    "        ds_nldas = xr.concat([_subset_centralUS(xr.open_dataset(file)[var_id]) for file in files], dim='time')\n",
    "    \n",
    "        # Select correct depth\n",
    "        if model in ['MOSAIC', 'NOAH']:\n",
    "            ds_nldas = ds_nldas.isel(depth=1)\n",
    "        else:\n",
    "            ds_nldas = ds_nldas.isel(depth=0)\n",
    "        \n",
    "        # Merge and store\n",
    "        ds_out = xr.Dataset({'soilMoist':ds_nldas})\n",
    "        ds_out.attrs['units'] = 'kg/m3'\n",
    "        ds_out.to_netcdf(f'{project_data_path}/WBM/precalibration/centralUS/{model}_validation.nc')\n",
    "\n",
    "        # Also store numpy array for quicker evaluations\n",
    "        npy_out = np.transpose(ds_out['soilMoist'].to_numpy(), (2,1,0))\n",
    "        np.save(f'{project_data_path}/WBM/precalibration/centralUS/{model}_validation.npy', npy_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a1bed8-40e2-4fab-bbdd-5280368685f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Climate forcing & geoophysical inputs ###\n",
    "#############################################\n",
    "for obs in ['MOSAIC', 'NOAH', 'VIC', 'SMAP']:\n",
    "    if not os.path.isfile(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/inputs.npz'):\n",
    "        # Climate drivers\n",
    "        if obs == \"SMAP\":\n",
    "            files = glob(f'{smap_path}/processed_nldas_grid/SMAP_L4_SM_gph_all_nldas_*.nc')\n",
    "            ds_forcing = xr.concat([_subset_centralUS(xr.open_dataset(file)) for file in files], dim='time')\n",
    "        else:\n",
    "            files = glob(f'{nldas_path}/forcing/daily/NLDAS_FORA0125_H.A*.nc')\n",
    "            ds_forcing = _subset_centralUS(xr.concat([xr.open_dataset(file) for file in files], dim='time'))\n",
    "\n",
    "        # Geophysical inputs\n",
    "        ds_lai = _subset_centralUS(xr.open_dataset(f'{project_data_path}/WBM/geo_inputs/LAI_clima_NLDASgrid.nc'))\n",
    "        if obs == 'VIC': # VIC does not provide awCap, wiltingp, so use NOAH\n",
    "            obs_soilp = 'NOAH'\n",
    "        else:\n",
    "            obs_soilp = obs\n",
    "        ds_awCap = _subset_centralUS(xr.open_dataset(f'{project_data_path}/WBM/geo_inputs/{obs_soilp}_awCap.nc'))\n",
    "        ds_wiltingp = _subset_centralUS(xr.open_dataset(f'{project_data_path}/WBM/geo_inputs/{obs_soilp}_wiltingp.nc'))\n",
    "    \n",
    "        ds_clayfrac = _subset_centralUS(xr.open_dataset(f'{project_data_path}/WBM/geo_inputs/clayfrac_NLDASgrid.nc'))\n",
    "        ds_sandfrac = _subset_centralUS(xr.open_dataset(f'{project_data_path}/WBM/geo_inputs/sandfrac_NLDASgrid.nc'))\n",
    "        ds_siltfrac = _subset_centralUS(xr.open_dataset(f'{project_data_path}/WBM/geo_inputs/siltfrac_NLDASgrid.nc'))\n",
    "    \n",
    "        # Initial conditions\n",
    "        ds_init = _subset_centralUS(xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/{obs}_validation.nc')).isel(time=0)\n",
    "    \n",
    "        # Numpy arrays in correct order\n",
    "        lats = ds_lai.lat.to_numpy()\n",
    "        doy = ds_forcing.time.dt.dayofyear.to_numpy() - 1\n",
    "    \n",
    "        if obs == \"SMAP\":\n",
    "            tas = np.transpose(ds_forcing['temp_lowatmmodlay'].to_numpy() - 273.15, (2,1,0))\n",
    "            prcp = np.transpose(ds_forcing['precipitation_total_surface_flux'].to_numpy() * 86400, (2,1,0))\n",
    "        else:\n",
    "            tas = np.transpose(ds_forcing['TMP'].to_numpy() - 273.15, (2,1,0))\n",
    "            prcp = np.transpose(ds_forcing['APCP'].to_numpy(), (2,1,0))\n",
    "    \n",
    "        lai = np.transpose(ds_lai['LAI'].to_numpy(), (2,1,0))\n",
    "        awCap = np.transpose(ds_awCap['awCap'].to_numpy())\n",
    "        wiltingp = np.transpose(ds_wiltingp['wiltingp'].to_numpy())\n",
    "    \n",
    "        clayfrac = np.transpose(ds_clayfrac['clayfrac'].to_numpy() / 100) # percentage -> fraction\n",
    "        sandfrac = np.transpose(ds_sandfrac['sandfrac'].to_numpy() / 100) # percentage -> fraction\n",
    "        siltfrac = np.transpose(ds_siltfrac['siltfrac'].to_numpy() / 100) # percentage -> fraction\n",
    "    \n",
    "        soilMoist_init = np.transpose(ds_init['soilMoist'].to_numpy())\n",
    "    \n",
    "        # Store numpy for easy access\n",
    "        np.savez(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/inputs.npz',\n",
    "                 tas=tas, prcp=prcp,\n",
    "                 lai=lai, awCap=awCap, wiltingp=wiltingp,\n",
    "                 lats=lats, doy=doy,\n",
    "                 clayfrac=clayfrac, sandfrac=sandfrac, siltfrac=siltfrac,\n",
    "                 soilMoist_init=soilMoist_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53df84-c550-4329-bbb2-dda941d2ef96",
   "metadata": {},
   "source": [
    "## Run ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea733adc-6adb-40f1-adf7-f508bfc42dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model code\n",
    "from water_balance import simulate_water_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04563f3e-a5e4-4f00-9e24-af5ba1573172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_WBMpy(forcing,\n",
    "              alpha_claycoef,\n",
    "              alpha_sandcoef,\n",
    "              alpha_siltcoef,\n",
    "              betaHBV_claycoef,\n",
    "              betaHBV_sandcoef,\n",
    "              betaHBV_siltcoef,\n",
    "              Ts,\n",
    "              Tm,\n",
    "              Kc,\n",
    "              awCap_factor,\n",
    "              # fieldCap_claycoef,\n",
    "              # fieldCap_sandcoef,\n",
    "              # fieldCap_siltcoef,\n",
    "              wiltingp_factor,\n",
    "              # wiltingp_claycoef,\n",
    "              # wiltingp_sandcoef,\n",
    "              # wiltingp_siltcoef,\n",
    "              Wi_init,\n",
    "              Sp_init,\n",
    "              GS_start,\n",
    "              GS_length,\n",
    "              rootDepth_oGS,\n",
    "              rootDepth_GS_factor,\n",
    "              iparam,\n",
    "              store_path,\n",
    "              simulate_water_balance):\n",
    "    \n",
    "    # Read and extract inputs\n",
    "    npz = np.load(forcing)\n",
    "    tas_in = npz['tas']\n",
    "    prcp_in = npz['prcp']\n",
    "    lai_in = npz['lai']\n",
    "    awCap = npz['awCap']\n",
    "    wiltingp = npz['wiltingp']\n",
    "    lats_in = npz['lats']\n",
    "    doy_in = npz['doy']\n",
    "    clayfrac = npz['clayfrac']\n",
    "    sandfrac = npz['sandfrac']\n",
    "    siltfrac = npz['siltfrac']\n",
    "    soilMoist_init = npz['soilMoist_init']\n",
    "    \n",
    "    # Array shapes\n",
    "    nlon, nlat = soilMoist_init.shape\n",
    "    grid = np.ones((nlon, nlat))\n",
    "\n",
    "    # Alpha and beta have lower bound of 1\n",
    "    alpha_in = 1. + (alpha_claycoef * clayfrac) + (alpha_sandcoef * sandfrac) + (alpha_siltcoef * siltfrac)\n",
    "    betaHBV_in = 1. + (betaHBV_claycoef * clayfrac) + (betaHBV_sandcoef * sandfrac) + (betaHBV_siltcoef * siltfrac)\n",
    "\n",
    "    # # field capacity in units of mm/m\n",
    "    # fieldCap = 1000 * ((fieldCap_claycoef * clayfrac) + (fieldCap_sandcoef * sandfrac) + (fieldCap_siltcoef * siltfrac))\n",
    "    # # wilting point is fraction of fieldCap\n",
    "    # wiltingp_in = fieldCap * ((wiltingp_claycoef * clayfrac) + (wiltingp_sandcoef * sandfrac) + (wiltingp_siltcoef * siltfrac))\n",
    "    # # construct awCap such that this relation holds\n",
    "    # awCap_in = fieldCap - wiltingp_in\n",
    "\n",
    "    # awCap\n",
    "    awCap_in = awCap_factor * awCap\n",
    "\n",
    "    # wiltingp\n",
    "    wiltingp_in = wiltingp_factor * wiltingp\n",
    "\n",
    "    # WBM models the active profile so let starting soil moisture reflect this\n",
    "    Ws_init_in = soilMoist_init - wiltingp_in\n",
    "    Ws_init_in = np.where(Ws_init_in >= 0., Ws_init_in, 0.)\n",
    "\n",
    "    # Other inputs\n",
    "    Kc_in = Kc * np.ones(lai_in.shape)\n",
    "    Wi_init_in = Wi_init * grid\n",
    "    Sp_init_in = Sp_init * grid\n",
    "    GS_start_in = GS_start * grid\n",
    "    GS_length_in = GS_length * grid\n",
    "    rootDepth_oGS_in = rootDepth_oGS * grid\n",
    "    rootDepth_GS_factor_in = rootDepth_GS_factor * grid\n",
    "\n",
    "    # Run it\n",
    "    out = simulate_water_balance(\n",
    "        Ws_init = Ws_init_in,\n",
    "        Wi_init = Wi_init_in,\n",
    "        Sp_init = Sp_init_in,\n",
    "        P = prcp_in,\n",
    "        T = tas_in,\n",
    "        Ts = Ts,\n",
    "        Tm = Tm,\n",
    "        rootDepth_oGS = rootDepth_oGS_in,\n",
    "        rootDepth_GS_factor = rootDepth_GS_factor_in,\n",
    "        awCap = awCap_in,\n",
    "        wilting_point = wiltingp_in,\n",
    "        GS_start = GS_start_in,\n",
    "        GS_length = GS_length_in,\n",
    "        lai = lai_in,\n",
    "        Kc = Kc_in,\n",
    "        alpha = alpha_in,\n",
    "        betaHBV = betaHBV_in,\n",
    "        phi = lats_in,\n",
    "        doy = doy_in\n",
    "    )\n",
    "\n",
    "    # Store\n",
    "    np.save(f'{store_path}/{str(iparam)}.npy', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3456c522-5079-46a4-be5b-7414565378a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ensemble\n",
    "df_param = pd.read_csv(f'{project_data_path}/WBM/precalibration/centralUS/params_extra.csv')\n",
    "\n",
    "# Constants\n",
    "Ts = -1.             # Snowfall threshold\n",
    "Tm = 1.              # Snowmelt threshold\n",
    "Wi_init = 0.         # Initial canopy water storage\n",
    "Sp_init = 0.         # Initial snowpack\n",
    "\n",
    "# Neglect influence of growing season for now\n",
    "GS_start = 0 # meaningless\n",
    "GS_length = 0 # meaningless\n",
    "rootDepth_oGS = 1000\n",
    "rootDepth_GS_factor = 1\n",
    "Kc = 1\n",
    "\n",
    "# Dask delayed\n",
    "delayed = []\n",
    "for obs in ['MOSAIC', 'NOAH', 'VIC', 'SMAP']:\n",
    "    for iparam in range(50_000, 60_000):\n",
    "        # Check if done\n",
    "        if not os.path.isfile(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/out/{str(iparam)}.npy'):\n",
    "        \n",
    "            # Get uncertain parameters\n",
    "            alpha_claycoef = df_param[df_param.iparam == iparam]['alpha_claycoef'].values[0]\n",
    "            alpha_sandcoef = df_param[df_param.iparam == iparam]['alpha_sandcoef'].values[0]\n",
    "            alpha_siltcoef = df_param[df_param.iparam == iparam]['alpha_siltcoef'].values[0]\n",
    "            \n",
    "            betaHBV_claycoef = df_param[df_param.iparam == iparam]['betaHBV_claycoef'].values[0]\n",
    "            betaHBV_sandcoef = df_param[df_param.iparam == iparam]['betaHBV_sandcoef'].values[0]\n",
    "            betaHBV_siltcoef = df_param[df_param.iparam == iparam]['betaHBV_siltcoef'].values[0]\n",
    "\n",
    "            awCap_factor = df_param[df_param.iparam == iparam]['awCap_factor'].values[0]\n",
    "            # fieldCap_claycoef = df_param[df_param.iparam == iparam]['fieldCap_claycoef'].values[0]\n",
    "            # fieldCap_sandcoef = df_param[df_param.iparam == iparam]['fieldCap_sandcoef'].values[0]\n",
    "            # fieldCap_siltcoef = df_param[df_param.iparam == iparam]['fieldCap_siltcoef'].values[0]\n",
    "\n",
    "            wiltingp_factor = df_param[df_param.iparam == iparam]['wiltingp_factor'].values[0]\n",
    "            # wiltingp_claycoef = df_param[df_param.iparam == iparam]['wiltingp_claycoef'].values[0]\n",
    "            # wiltingp_sandcoef = df_param[df_param.iparam == iparam]['wiltingp_sandcoef'].values[0]\n",
    "            # wiltingp_siltcoef = df_param[df_param.iparam == iparam]['wiltingp_siltcoef'].values[0]\n",
    "\n",
    "            # Delayed calculation\n",
    "            tmp = dask.delayed(run_WBMpy)(\n",
    "                forcing = f'{project_data_path}/WBM/precalibration/centralUS/{obs}/inputs.npz',\n",
    "                alpha_claycoef = alpha_claycoef,\n",
    "                alpha_sandcoef = alpha_sandcoef,\n",
    "                alpha_siltcoef = alpha_siltcoef,\n",
    "                betaHBV_claycoef = betaHBV_claycoef,\n",
    "                betaHBV_sandcoef = betaHBV_sandcoef,\n",
    "                betaHBV_siltcoef = betaHBV_siltcoef,\n",
    "                awCap_factor = awCap_factor,\n",
    "                wiltingp_factor = wiltingp_factor,\n",
    "                # fieldCap_claycoef = fieldCap_claycoef,\n",
    "                # fieldCap_sandcoef = fieldCap_sandcoef,\n",
    "                # fieldCap_siltcoef = fieldCap_siltcoef,\n",
    "                # wiltingp_claycoef = wiltingp_claycoef,\n",
    "                # wiltingp_sandcoef = wiltingp_sandcoef,\n",
    "                # wiltingp_siltcoef = wiltingp_siltcoef,\n",
    "                Ts = Ts,\n",
    "                Tm = Tm,\n",
    "                Kc = Kc,\n",
    "                Wi_init = Wi_init,\n",
    "                Sp_init = Sp_init,\n",
    "                GS_start = GS_start,\n",
    "                GS_length = GS_length,\n",
    "                rootDepth_oGS = rootDepth_oGS,\n",
    "                rootDepth_GS_factor = rootDepth_GS_factor,\n",
    "                iparam = iparam,\n",
    "                store_path = f'{project_data_path}/WBM/precalibration/centralUS/{obs}/out',\n",
    "                simulate_water_balance = simulate_water_balance\n",
    "            )\n",
    "\n",
    "            delayed.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863a12cc-c447-4375-b38f-aa81a7da060d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab5cefd-2f66-4a7a-9028-679baedad133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 31.66 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 27s, sys: 2min 19s, total: 46min 47s\n",
      "Wall time: 7h 27min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute\n",
    "results = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcfec66-11b0-4981-81a8-ca24762ec84f",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761faa96-8da2-46a7-a17a-b871e9801401",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35886a71-1ddf-44d3-8839-b81827e70c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pearson correlation and RMSE\n",
    "def calculate_rmse(obs, iparam, tmin, project_data_path, log_path):\n",
    "    if os.path.isfile(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/out/{str(iparam)}.npy'):\n",
    "        # Read simulation output\n",
    "        sim_tmp = np.load(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/out/{str(iparam)}.npy')\n",
    "\n",
    "        # Read obs\n",
    "        ds_obs = np.load(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/{obs}_validation.npy')\n",
    "\n",
    "        # RMSE\n",
    "        def calculate_rmse_global(predictions, targets):\n",
    "            return np.sqrt(np.nanmean((predictions-targets)**2))\n",
    "\n",
    "        def calculate_rmse_gpa(predictions, targets):\n",
    "            # Assumes time axis is -1\n",
    "            return np.nanmean(np.sqrt(np.nanmean(((predictions-targets)**2), axis=-1)))\n",
    "\n",
    "        def calculate_rmse_ngpa(predictions, targets):\n",
    "            # Assumes time axis is -1\n",
    "            rmse = np.sqrt(np.nanmean(((predictions-targets)**2), axis=-1))\n",
    "            mean = np.nanmean(targets, axis=-1)\n",
    "            return np.nanmean(rmse/mean)\n",
    "\n",
    "        # Skip first tmin timesteps for spinup\n",
    "        rmse_gpa = calculate_rmse_gpa(sim_tmp[:,:,tmin:], ds_obs[:,:,tmin:])\n",
    "        # rmse_ngpa = calculate_rmse_ngpa(sim_tmp[:,:,tmin:], ds_obs[:,:,tmin:])\n",
    "\n",
    "        # Merge all\n",
    "        df_out = pd.DataFrame(data = {\n",
    "            'rmse_gpa': [rmse_gpa],\n",
    "            # 'rmse_ngpa': [rmse_ngpa],\n",
    "            'iparam': [iparam],\n",
    "        })\n",
    "        \n",
    "        # Return spatial average\n",
    "        return df_out\n",
    "    else:\n",
    "        return None\n",
    "            \n",
    "    # except Exception as e:\n",
    "    #      with open(f'{log_path}/rmse_{obs}_{str(iparam)}.txt', 'w') as f:\n",
    "    #         f.write(str(e))\n",
    "    #         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "698b6f07-50fc-4de2-9147-e860f5ba8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 10.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 10.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 10.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 10.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 15s, sys: 35.8 s, total: 13min 51s\n",
      "Wall time: 1h 21min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Skip first tmin days\n",
    "tmin = 0\n",
    "\n",
    "for obs in ['NOAH', 'MOSAIC', 'VIC', 'SMAP']:\n",
    "    # Delayed\n",
    "    delayed = []\n",
    "\n",
    "    for iparam in range(60_000):\n",
    "        delayed.append(dask.delayed(calculate_rmse)(obs = obs,\n",
    "                                                    iparam = iparam,\n",
    "                                                    tmin = tmin,\n",
    "                                                    project_data_path = project_data_path,\n",
    "                                                    log_path = log_path))\n",
    "\n",
    "    # Compute\n",
    "    out = dask.compute(*delayed)\n",
    "\n",
    "    # Store (append if any new results)\n",
    "    out_path = f'{project_data_path}/WBM/precalibration/centralUS/{obs}/soilMoist_rmse_skip{str(tmin)}.csv'\n",
    "    if os.path.isfile(out_path):\n",
    "        # Read existing\n",
    "        df_res = pd.read_csv(out_path)\n",
    "        # Append new\n",
    "        df_out = pd.concat(out, ignore_index=True)\n",
    "        df_res = pd.concat([df_res, df_out], ignore_index=True)\n",
    "        # Drop duplicates\n",
    "        df_res = df_res.drop_duplicates(subset='iparam')\n",
    "        # Store\n",
    "        df_res.to_csv(out_path, index=False)\n",
    "    else:\n",
    "        df_out = pd.concat(out, ignore_index=True)\n",
    "        df_out.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f65229-6dc2-45f3-8e5c-853bb0bd92d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f4e8c-fb70-4670-805b-6060503717f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## OLD  \n",
    "# Use weekly anomalies to validate across different obs\n",
    "# ds_nldas = ds_nldas.assign_coords(week=ds_nldas.time.dt.strftime(\"%W\"))\n",
    "# ds_nldas_anom = (ds_nldas.groupby('week') - ds_nldas.groupby(\"week\").mean(\"time\"))\n",
    "\n",
    "# np.save(f'{project_data_path}/WBM/precalibration/centralUS/{model}_obs_anoms_weekly.npy',\n",
    "#         np.transpose(ds_nldas_anom[var_id].to_numpy(), (2,1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e873955-b291-4327-a68e-7e8cd3dc9147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pearson correlation and RMSE\n",
    "def calculate_error_metrics(project_data_path, iparam):\n",
    "    try:\n",
    "        # Read simulation output\n",
    "        sim_tmp = np.load(f'{project_data_path}/WBM/precalibration/centralUS/out/{str(iparam)}_Ws.npy')\n",
    "\n",
    "        # Read SMAP for coords\n",
    "        smap_obs = xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/SMAP_validation.nc').drop(['soilMoist_anom', 'week'])\n",
    "\n",
    "        # Construct xr \n",
    "        ds_sim = xr.Dataset(\n",
    "            data_vars=dict(soilMoist=([\"time\", \"lat\", \"lon\"], np.transpose(sim_tmp, (2,1,0)))),\n",
    "            coords=dict(\n",
    "                lon=smap_obs.lon,\n",
    "                lat=smap_obs.lat,\n",
    "                time=smap_obs.time)\n",
    "        )\n",
    "\n",
    "        # Get anomaly from weekly climatology\n",
    "        ds_sim['soilMoist_clima'] = ds_sim['soilMoist'].groupby(ds_sim.time.dt.isocalendar().week).mean(\"time\")\n",
    "        ds_sim['soilMoist_anom'] = (ds_sim['soilMoist'].groupby(ds_sim.time.dt.isocalendar().week) - ds_sim['soilMoist_clima'])\n",
    "        \n",
    "        smap_obs['soilMoist_clima'] = smap_obs['soilMoist'].groupby(smap_obs.time.dt.isocalendar().week).mean(\"time\")\n",
    "        smap_obs['soilMoist_anom'] = (smap_obs['soilMoist'].groupby(smap_obs.time.dt.isocalendar().week) - smap_obs['soilMoist_clima'])\n",
    "        \n",
    "        # Read other observations\n",
    "        def calculate_weekly_clima(ds):\n",
    "            ds_tmp = ds.copy().drop(['soilMoist_anom', 'week'])\n",
    "            ds_tmp['soilMoist_clima'] = ds_tmp['soilMoist'].groupby(ds_tmp.time.dt.isocalendar().week).mean(\"time\")\n",
    "            ds_tmp['soilMoist_anom'] = (ds_tmp['soilMoist'].groupby(ds_tmp.time.dt.isocalendar().week) - ds_tmp['soilMoist_clima'])\n",
    "            return ds_tmp\n",
    "            \n",
    "        noah_obs = calculate_weekly_clima(xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/NOAH_validation.nc'))\n",
    "        vic_obs = calculate_weekly_clima(xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/VIC_validation.nc'))\n",
    "        mosaic_obs = calculate_weekly_clima(xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/MOSAIC_validation.nc'))\n",
    "\n",
    "        # Person correlation\n",
    "        # smap_corr = xr.corr(ds_sim['soilMoist'].isel(time=slice(100,3000)), smap_obs['soilMoist'].isel(time=slice(100,3000)), dim='time')\n",
    "        # vic_corr = xr.corr(ds_sim['soilMoist'].isel(time=slice(100,3000)), vic_obs['soilMoist'].isel(time=slice(100,3000)), dim='time')\n",
    "        # mosaic_corr = xr.corr(ds_sim['soilMoist'].isel(time=slice(100,3000)), mosaic_obs['soilMoist'].isel(time=slice(100,3000)), dim='time')\n",
    "        # noah_corr = xr.corr(ds_sim['soilMoist'].isel(time=slice(100,3000)), noah_obs['soilMoist'].isel(time=slice(100,3000)), dim='time')\n",
    "\n",
    "        # smap_corr = xr.corr(ds_sim['soilMoist_clima'], smap_obs['soilMoist_clima'], dim='time')\n",
    "        # vic_corr = xr.corr(ds_sim['soilMoist_clima'], vic_obs['soilMoist_clima'], dim='time')\n",
    "        # mosaic_corr = xr.corr(ds_sim['soilMoist_clima'], mosaic_obs['soilMoist_clima'], dim='time')\n",
    "        # noah_corr = xr.corr(ds_sim['soilMoist_clima'], noah_obs['soilMoist_clima'], dim='time')\n",
    "        \n",
    "        smap_corr_anom = xr.corr(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), smap_obs['soilMoist_anom'].isel(time=slice(100,3000)), dim='time')\n",
    "        vic_corr_anom = xr.corr(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), vic_obs['soilMoist_anom'].isel(time=slice(100,3000)), dim='time')\n",
    "        mosaic_corr_anom = xr.corr(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), mosaic_obs['soilMoist_anom'].isel(time=slice(100,3000)), dim='time')\n",
    "        noah_corr_anom = xr.corr(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), noah_obs['soilMoist_anom'].isel(time=slice(100,3000)), dim='time')\n",
    "\n",
    "        # RMSE\n",
    "        def _calculate_rmse(ds1, ds2):\n",
    "            return np.sqrt(((ds1 - ds2)**2).mean(dim='week'))\n",
    "\n",
    "        # smap_rmse = _calculate_rmse(ds_sim['soilMoist'].isel(time=slice(100,3000)), smap_obs['soilMoist'].isel(time=slice(100,3000)))\n",
    "        # vic_rmse = _calculate_rmse(ds_sim['soilMoist'].isel(time=slice(100,3000)), vic_obs['soilMoist'].isel(time=slice(100,3000)))\n",
    "        # mosaic_rmse = _calculate_rmse(ds_sim['soilMoist'].isel(time=slice(100,3000)), mosaic_obs['soilMoist'].isel(time=slice(100,3000)))\n",
    "        # noah_rmse = _calculate_rmse(ds_sim['soilMoist'].isel(time=slice(100,3000)), noah_obs['soilMoist'].isel(time=slice(100,3000)))\n",
    "\n",
    "        smap_rmse = _calculate_rmse(ds_sim['soilMoist_clima'], smap_obs['soilMoist_clima'])\n",
    "        vic_rmse = _calculate_rmse(ds_sim['soilMoist_clima'], vic_obs['soilMoist_clima'])\n",
    "        mosaic_rmse = _calculate_rmse(ds_sim['soilMoist_clima'], mosaic_obs['soilMoist_clima'])\n",
    "        noah_rmse = _calculate_rmse(ds_sim['soilMoist_clima'], noah_obs['soilMoist_clima'])\n",
    "        \n",
    "        # smap_rmse_anom = _calculate_rmse(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), smap_obs['soilMoist_anom'].isel(time=slice(100,3000)))\n",
    "        # vic_rmse_anom = _calculate_rmse(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), vic_obs['soilMoist_anom'].isel(time=slice(100,3000)))\n",
    "        # mosaic_rmse_anom = _calculate_rmse(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), mosaic_obs['soilMoist_anom'].isel(time=slice(100,3000)))\n",
    "        # noah_rmse_anom = _calculate_rmse(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), noah_obs['soilMoist_anom'].isel(time=slice(100,3000)))\n",
    "\n",
    "        # Merge all\n",
    "        ds_out = xr.Dataset({\n",
    "            # 'smap_pearsonr': smap_corr,\n",
    "            'smap_pearsonr_anom': smap_corr_anom,\n",
    "            # 'vic_pearsonr': vic_corr,\n",
    "            'vic_pearsonr_anom': vic_corr_anom,\n",
    "            # 'noah_pearsonr': noah_corr,\n",
    "            'noah_pearsonr_anom': noah_corr_anom,\n",
    "            # 'mosaic_pearsonr': mosaic_corr,\n",
    "            'mosaic_pearsonr_anom': mosaic_corr_anom,\n",
    "            'smap_rmse_clima': smap_rmse,\n",
    "            # 'smap_rmse_anom': smap_rmse_anom,\n",
    "            'vic_rmse_clima': vic_rmse,\n",
    "            # 'vic_rmse_anom': vic_rmse_anom,\n",
    "            'noah_rmse_clima': noah_rmse,\n",
    "            # 'noah_rmse_anom': noah_rmse_anom,\n",
    "            'mosaic_rmse_clima': mosaic_rmse,\n",
    "            # 'mosaic_rmse_anom': mosaic_rmse_anom,\n",
    "        })\n",
    "        \n",
    "        # Return spatial average\n",
    "        return ds_out.mean(dim=['lat','lon']).assign_coords(iparam=[iparam]).to_dataframe()\n",
    "            \n",
    "    except Exception as e:\n",
    "         with open(f'/storage/work/dcl5300/current_projects/wbm_soilM_crop_uc_lafferty-etal-2024-tbd/code/logs/{str(iparam)}.txt', 'w') as f:\n",
    "            f.write(str(e))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27969869-158c-4919-9372-b5987caed7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delayed\n",
    "delayed = []\n",
    "\n",
    "for iparam in range(10_000):\n",
    "    tmp = dask.delayed(calculate_error_metrics)(project_data_path = project_data_path,\n",
    "                                                iparam = iparam)\n",
    "    delayed.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bebaa80-cf7a-4438-b816-49b2574b9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute\n",
    "out = dask.compute(*delayed)\n",
    "\n",
    "# Store\n",
    "pd.concat(out).to_csv(f'{project_data_path}/WBM/precalibration/centralUS/results_2step.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40da33-b55b-4ed3-bd25-ad93c683df86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
