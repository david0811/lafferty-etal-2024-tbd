{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf53df84-c550-4329-bbb2-dda941d2ef96",
   "metadata": {},
   "source": [
    "## Run ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea733adc-6adb-40f1-adf7-f508bfc42dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model code\n",
    "from water_balance import simulate_water_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04563f3e-a5e4-4f00-9e24-af5ba1573172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_WBMpy(forcing,\n",
    "              alpha_claycoef,\n",
    "              alpha_sandcoef,\n",
    "              alpha_siltcoef,\n",
    "              betaHBV_claycoef,\n",
    "              betaHBV_sandcoef,\n",
    "              betaHBV_siltcoef,\n",
    "              Ts,\n",
    "              Tm,\n",
    "              Kc,\n",
    "              awCap_factor,\n",
    "              # fieldCap_claycoef,\n",
    "              # fieldCap_sandcoef,\n",
    "              # fieldCap_siltcoef,\n",
    "              wiltingp_factor,\n",
    "              # wiltingp_claycoef,\n",
    "              # wiltingp_sandcoef,\n",
    "              # wiltingp_siltcoef,\n",
    "              Wi_init,\n",
    "              Sp_init,\n",
    "              GS_start,\n",
    "              GS_length,\n",
    "              rootDepth_oGS,\n",
    "              rootDepth_GS_factor,\n",
    "              iparam,\n",
    "              store_path,\n",
    "              simulate_water_balance):\n",
    "    \n",
    "    # Read and extract inputs\n",
    "    npz = np.load(forcing)\n",
    "    tas_in = npz['tas']\n",
    "    prcp_in = npz['prcp']\n",
    "    lai_in = npz['lai']\n",
    "    awCap = npz['awCap']\n",
    "    wiltingp = npz['wiltingp']\n",
    "    lats_in = npz['lats']\n",
    "    doy_in = npz['doy']\n",
    "    clayfrac = npz['clayfrac']\n",
    "    sandfrac = npz['sandfrac']\n",
    "    siltfrac = npz['siltfrac']\n",
    "    soilMoist_init = npz['soilMoist_init']\n",
    "    \n",
    "    # Array shapes\n",
    "    nlon, nlat = soilMoist_init.shape\n",
    "    grid = np.ones((nlon, nlat))\n",
    "\n",
    "    # Alpha and beta have lower bound of 1\n",
    "    alpha_in = 1. + (alpha_claycoef * clayfrac) + (alpha_sandcoef * sandfrac) + (alpha_siltcoef * siltfrac)\n",
    "    betaHBV_in = 1. + (betaHBV_claycoef * clayfrac) + (betaHBV_sandcoef * sandfrac) + (betaHBV_siltcoef * siltfrac)\n",
    "\n",
    "    # # field capacity in units of mm/m\n",
    "    # fieldCap = 1000 * ((fieldCap_claycoef * clayfrac) + (fieldCap_sandcoef * sandfrac) + (fieldCap_siltcoef * siltfrac))\n",
    "    # # wilting point is fraction of fieldCap\n",
    "    # wiltingp_in = fieldCap * ((wiltingp_claycoef * clayfrac) + (wiltingp_sandcoef * sandfrac) + (wiltingp_siltcoef * siltfrac))\n",
    "    # # construct awCap such that this relation holds\n",
    "    # awCap_in = fieldCap - wiltingp_in\n",
    "\n",
    "    # awCap\n",
    "    awCap_in = awCap_factor * awCap\n",
    "\n",
    "    # wiltingp\n",
    "    wiltingp_in = wiltingp_factor * wiltingp\n",
    "\n",
    "    # WBM models the active profile so let starting soil moisture reflect this\n",
    "    Ws_init_in = soilMoist_init - wiltingp_in\n",
    "    Ws_init_in = np.where(Ws_init_in >= 0., Ws_init_in, 0.)\n",
    "\n",
    "    # Other inputs\n",
    "    Kc_in = Kc * np.ones(lai_in.shape)\n",
    "    Wi_init_in = Wi_init * grid\n",
    "    Sp_init_in = Sp_init * grid\n",
    "    GS_start_in = GS_start * grid\n",
    "    GS_length_in = GS_length * grid\n",
    "    rootDepth_oGS_in = rootDepth_oGS * grid\n",
    "    rootDepth_GS_factor_in = rootDepth_GS_factor * grid\n",
    "\n",
    "    # Run it\n",
    "    out = simulate_water_balance(\n",
    "        Ws_init = Ws_init_in,\n",
    "        Wi_init = Wi_init_in,\n",
    "        Sp_init = Sp_init_in,\n",
    "        P = prcp_in,\n",
    "        T = tas_in,\n",
    "        Ts = Ts,\n",
    "        Tm = Tm,\n",
    "        rootDepth_oGS = rootDepth_oGS_in,\n",
    "        rootDepth_GS_factor = rootDepth_GS_factor_in,\n",
    "        awCap = awCap_in,\n",
    "        wilting_point = wiltingp_in,\n",
    "        GS_start = GS_start_in,\n",
    "        GS_length = GS_length_in,\n",
    "        lai = lai_in,\n",
    "        Kc = Kc_in,\n",
    "        alpha = alpha_in,\n",
    "        betaHBV = betaHBV_in,\n",
    "        phi = lats_in,\n",
    "        doy = doy_in\n",
    "    )\n",
    "\n",
    "    # Store\n",
    "    np.save(f'{store_path}/{str(iparam)}.npy', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3456c522-5079-46a4-be5b-7414565378a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ensemble\n",
    "df_param = pd.read_csv(f'{project_data_path}/WBM/precalibration/centralUS/params_extra.csv')\n",
    "\n",
    "# Constants\n",
    "Ts = -1.             # Snowfall threshold\n",
    "Tm = 1.              # Snowmelt threshold\n",
    "Wi_init = 0.         # Initial canopy water storage\n",
    "Sp_init = 0.         # Initial snowpack\n",
    "\n",
    "# Neglect influence of growing season for now\n",
    "GS_start = 0 # meaningless\n",
    "GS_length = 0 # meaningless\n",
    "rootDepth_oGS = 1000\n",
    "rootDepth_GS_factor = 1\n",
    "Kc = 1\n",
    "\n",
    "# Dask delayed\n",
    "delayed = []\n",
    "for obs in ['MOSAIC', 'NOAH', 'VIC', 'SMAP']:\n",
    "    for iparam in range(50_000, 60_000):\n",
    "        # Check if done\n",
    "        if not os.path.isfile(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/out/{str(iparam)}.npy'):\n",
    "        \n",
    "            # Get uncertain parameters\n",
    "            alpha_claycoef = df_param[df_param.iparam == iparam]['alpha_claycoef'].values[0]\n",
    "            alpha_sandcoef = df_param[df_param.iparam == iparam]['alpha_sandcoef'].values[0]\n",
    "            alpha_siltcoef = df_param[df_param.iparam == iparam]['alpha_siltcoef'].values[0]\n",
    "            \n",
    "            betaHBV_claycoef = df_param[df_param.iparam == iparam]['betaHBV_claycoef'].values[0]\n",
    "            betaHBV_sandcoef = df_param[df_param.iparam == iparam]['betaHBV_sandcoef'].values[0]\n",
    "            betaHBV_siltcoef = df_param[df_param.iparam == iparam]['betaHBV_siltcoef'].values[0]\n",
    "\n",
    "            awCap_factor = df_param[df_param.iparam == iparam]['awCap_factor'].values[0]\n",
    "            # fieldCap_claycoef = df_param[df_param.iparam == iparam]['fieldCap_claycoef'].values[0]\n",
    "            # fieldCap_sandcoef = df_param[df_param.iparam == iparam]['fieldCap_sandcoef'].values[0]\n",
    "            # fieldCap_siltcoef = df_param[df_param.iparam == iparam]['fieldCap_siltcoef'].values[0]\n",
    "\n",
    "            wiltingp_factor = df_param[df_param.iparam == iparam]['wiltingp_factor'].values[0]\n",
    "            # wiltingp_claycoef = df_param[df_param.iparam == iparam]['wiltingp_claycoef'].values[0]\n",
    "            # wiltingp_sandcoef = df_param[df_param.iparam == iparam]['wiltingp_sandcoef'].values[0]\n",
    "            # wiltingp_siltcoef = df_param[df_param.iparam == iparam]['wiltingp_siltcoef'].values[0]\n",
    "\n",
    "            # Delayed calculation\n",
    "            tmp = dask.delayed(run_WBMpy)(\n",
    "                forcing = f'{project_data_path}/WBM/precalibration/centralUS/{obs}/inputs.npz',\n",
    "                alpha_claycoef = alpha_claycoef,\n",
    "                alpha_sandcoef = alpha_sandcoef,\n",
    "                alpha_siltcoef = alpha_siltcoef,\n",
    "                betaHBV_claycoef = betaHBV_claycoef,\n",
    "                betaHBV_sandcoef = betaHBV_sandcoef,\n",
    "                betaHBV_siltcoef = betaHBV_siltcoef,\n",
    "                awCap_factor = awCap_factor,\n",
    "                wiltingp_factor = wiltingp_factor,\n",
    "                # fieldCap_claycoef = fieldCap_claycoef,\n",
    "                # fieldCap_sandcoef = fieldCap_sandcoef,\n",
    "                # fieldCap_siltcoef = fieldCap_siltcoef,\n",
    "                # wiltingp_claycoef = wiltingp_claycoef,\n",
    "                # wiltingp_sandcoef = wiltingp_sandcoef,\n",
    "                # wiltingp_siltcoef = wiltingp_siltcoef,\n",
    "                Ts = Ts,\n",
    "                Tm = Tm,\n",
    "                Kc = Kc,\n",
    "                Wi_init = Wi_init,\n",
    "                Sp_init = Sp_init,\n",
    "                GS_start = GS_start,\n",
    "                GS_length = GS_length,\n",
    "                rootDepth_oGS = rootDepth_oGS,\n",
    "                rootDepth_GS_factor = rootDepth_GS_factor,\n",
    "                iparam = iparam,\n",
    "                store_path = f'{project_data_path}/WBM/precalibration/centralUS/{obs}/out',\n",
    "                simulate_water_balance = simulate_water_balance\n",
    "            )\n",
    "\n",
    "            delayed.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863a12cc-c447-4375-b38f-aa81a7da060d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab5cefd-2f66-4a7a-9028-679baedad133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 31.66 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 27s, sys: 2min 19s, total: 46min 47s\n",
      "Wall time: 7h 27min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute\n",
    "results = dask.compute(*delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcfec66-11b0-4981-81a8-ca24762ec84f",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761faa96-8da2-46a7-a17a-b871e9801401",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35886a71-1ddf-44d3-8839-b81827e70c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pearson correlation and RMSE\n",
    "def calculate_rmse(obs, iparam, tmin, project_data_path, log_path):\n",
    "    if os.path.isfile(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/out/{str(iparam)}.npy'):\n",
    "        # Read simulation output\n",
    "        sim_tmp = np.load(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/out/{str(iparam)}.npy')\n",
    "\n",
    "        # Read obs\n",
    "        ds_obs = np.load(f'{project_data_path}/WBM/precalibration/centralUS/{obs}/{obs}_validation.npy')\n",
    "\n",
    "        # RMSE\n",
    "        def calculate_rmse_global(predictions, targets):\n",
    "            return np.sqrt(np.nanmean((predictions-targets)**2))\n",
    "\n",
    "        def calculate_rmse_gpa(predictions, targets):\n",
    "            # Assumes time axis is -1\n",
    "            return np.nanmean(np.sqrt(np.nanmean(((predictions-targets)**2), axis=-1)))\n",
    "\n",
    "        def calculate_rmse_ngpa(predictions, targets):\n",
    "            # Assumes time axis is -1\n",
    "            rmse = np.sqrt(np.nanmean(((predictions-targets)**2), axis=-1))\n",
    "            mean = np.nanmean(targets, axis=-1)\n",
    "            return np.nanmean(rmse/mean)\n",
    "\n",
    "        # Skip first tmin timesteps for spinup\n",
    "        rmse_gpa = calculate_rmse_gpa(sim_tmp[:,:,tmin:], ds_obs[:,:,tmin:])\n",
    "        # rmse_ngpa = calculate_rmse_ngpa(sim_tmp[:,:,tmin:], ds_obs[:,:,tmin:])\n",
    "\n",
    "        # Merge all\n",
    "        df_out = pd.DataFrame(data = {\n",
    "            'rmse_gpa': [rmse_gpa],\n",
    "            # 'rmse_ngpa': [rmse_ngpa],\n",
    "            'iparam': [iparam],\n",
    "        })\n",
    "        \n",
    "        # Return spatial average\n",
    "        return df_out\n",
    "    else:\n",
    "        return None\n",
    "            \n",
    "    # except Exception as e:\n",
    "    #      with open(f'{log_path}/rmse_{obs}_{str(iparam)}.txt', 'w') as f:\n",
    "    #         f.write(str(e))\n",
    "    #         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "698b6f07-50fc-4de2-9147-e860f5ba8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 10.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 10.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 10.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/storage/home/dcl5300/miniforge3/envs/climate-stack-mamba-2023-11-01/lib/python3.11/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 10.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 15s, sys: 35.8 s, total: 13min 51s\n",
      "Wall time: 1h 21min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Skip first tmin days\n",
    "tmin = 0\n",
    "\n",
    "for obs in ['NOAH', 'MOSAIC', 'VIC', 'SMAP']:\n",
    "    # Delayed\n",
    "    delayed = []\n",
    "\n",
    "    for iparam in range(60_000):\n",
    "        delayed.append(dask.delayed(calculate_rmse)(obs = obs,\n",
    "                                                    iparam = iparam,\n",
    "                                                    tmin = tmin,\n",
    "                                                    project_data_path = project_data_path,\n",
    "                                                    log_path = log_path))\n",
    "\n",
    "    # Compute\n",
    "    out = dask.compute(*delayed)\n",
    "\n",
    "    # Store (append if any new results)\n",
    "    out_path = f'{project_data_path}/WBM/precalibration/centralUS/{obs}/soilMoist_rmse_skip{str(tmin)}.csv'\n",
    "    if os.path.isfile(out_path):\n",
    "        # Read existing\n",
    "        df_res = pd.read_csv(out_path)\n",
    "        # Append new\n",
    "        df_out = pd.concat(out, ignore_index=True)\n",
    "        df_res = pd.concat([df_res, df_out], ignore_index=True)\n",
    "        # Drop duplicates\n",
    "        df_res = df_res.drop_duplicates(subset='iparam')\n",
    "        # Store\n",
    "        df_res.to_csv(out_path, index=False)\n",
    "    else:\n",
    "        df_out = pd.concat(out, ignore_index=True)\n",
    "        df_out.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f65229-6dc2-45f3-8e5c-853bb0bd92d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f4e8c-fb70-4670-805b-6060503717f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## OLD  \n",
    "# Use weekly anomalies to validate across different obs\n",
    "# ds_nldas = ds_nldas.assign_coords(week=ds_nldas.time.dt.strftime(\"%W\"))\n",
    "# ds_nldas_anom = (ds_nldas.groupby('week') - ds_nldas.groupby(\"week\").mean(\"time\"))\n",
    "\n",
    "# np.save(f'{project_data_path}/WBM/precalibration/centralUS/{model}_obs_anoms_weekly.npy',\n",
    "#         np.transpose(ds_nldas_anom[var_id].to_numpy(), (2,1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e873955-b291-4327-a68e-7e8cd3dc9147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pearson correlation and RMSE\n",
    "def calculate_error_metrics(project_data_path, iparam):\n",
    "    try:\n",
    "        # Read simulation output\n",
    "        sim_tmp = np.load(f'{project_data_path}/WBM/precalibration/centralUS/out/{str(iparam)}_Ws.npy')\n",
    "\n",
    "        # Read SMAP for coords\n",
    "        smap_obs = xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/SMAP_validation.nc').drop(['soilMoist_anom', 'week'])\n",
    "\n",
    "        # Construct xr \n",
    "        ds_sim = xr.Dataset(\n",
    "            data_vars=dict(soilMoist=([\"time\", \"lat\", \"lon\"], np.transpose(sim_tmp, (2,1,0)))),\n",
    "            coords=dict(\n",
    "                lon=smap_obs.lon,\n",
    "                lat=smap_obs.lat,\n",
    "                time=smap_obs.time)\n",
    "        )\n",
    "\n",
    "        # Get anomaly from weekly climatology\n",
    "        ds_sim['soilMoist_clima'] = ds_sim['soilMoist'].groupby(ds_sim.time.dt.isocalendar().week).mean(\"time\")\n",
    "        ds_sim['soilMoist_anom'] = (ds_sim['soilMoist'].groupby(ds_sim.time.dt.isocalendar().week) - ds_sim['soilMoist_clima'])\n",
    "        \n",
    "        smap_obs['soilMoist_clima'] = smap_obs['soilMoist'].groupby(smap_obs.time.dt.isocalendar().week).mean(\"time\")\n",
    "        smap_obs['soilMoist_anom'] = (smap_obs['soilMoist'].groupby(smap_obs.time.dt.isocalendar().week) - smap_obs['soilMoist_clima'])\n",
    "        \n",
    "        # Read other observations\n",
    "        def calculate_weekly_clima(ds):\n",
    "            ds_tmp = ds.copy().drop(['soilMoist_anom', 'week'])\n",
    "            ds_tmp['soilMoist_clima'] = ds_tmp['soilMoist'].groupby(ds_tmp.time.dt.isocalendar().week).mean(\"time\")\n",
    "            ds_tmp['soilMoist_anom'] = (ds_tmp['soilMoist'].groupby(ds_tmp.time.dt.isocalendar().week) - ds_tmp['soilMoist_clima'])\n",
    "            return ds_tmp\n",
    "            \n",
    "        noah_obs = calculate_weekly_clima(xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/NOAH_validation.nc'))\n",
    "        vic_obs = calculate_weekly_clima(xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/VIC_validation.nc'))\n",
    "        mosaic_obs = calculate_weekly_clima(xr.open_dataset(f'{project_data_path}/WBM/precalibration/centralUS/MOSAIC_validation.nc'))\n",
    "\n",
    "        # Person correlation\n",
    "        # smap_corr = xr.corr(ds_sim['soilMoist'].isel(time=slice(100,3000)), smap_obs['soilMoist'].isel(time=slice(100,3000)), dim='time')\n",
    "        # vic_corr = xr.corr(ds_sim['soilMoist'].isel(time=slice(100,3000)), vic_obs['soilMoist'].isel(time=slice(100,3000)), dim='time')\n",
    "        # mosaic_corr = xr.corr(ds_sim['soilMoist'].isel(time=slice(100,3000)), mosaic_obs['soilMoist'].isel(time=slice(100,3000)), dim='time')\n",
    "        # noah_corr = xr.corr(ds_sim['soilMoist'].isel(time=slice(100,3000)), noah_obs['soilMoist'].isel(time=slice(100,3000)), dim='time')\n",
    "\n",
    "        # smap_corr = xr.corr(ds_sim['soilMoist_clima'], smap_obs['soilMoist_clima'], dim='time')\n",
    "        # vic_corr = xr.corr(ds_sim['soilMoist_clima'], vic_obs['soilMoist_clima'], dim='time')\n",
    "        # mosaic_corr = xr.corr(ds_sim['soilMoist_clima'], mosaic_obs['soilMoist_clima'], dim='time')\n",
    "        # noah_corr = xr.corr(ds_sim['soilMoist_clima'], noah_obs['soilMoist_clima'], dim='time')\n",
    "        \n",
    "        smap_corr_anom = xr.corr(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), smap_obs['soilMoist_anom'].isel(time=slice(100,3000)), dim='time')\n",
    "        vic_corr_anom = xr.corr(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), vic_obs['soilMoist_anom'].isel(time=slice(100,3000)), dim='time')\n",
    "        mosaic_corr_anom = xr.corr(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), mosaic_obs['soilMoist_anom'].isel(time=slice(100,3000)), dim='time')\n",
    "        noah_corr_anom = xr.corr(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), noah_obs['soilMoist_anom'].isel(time=slice(100,3000)), dim='time')\n",
    "\n",
    "        # RMSE\n",
    "        def _calculate_rmse(ds1, ds2):\n",
    "            return np.sqrt(((ds1 - ds2)**2).mean(dim='week'))\n",
    "\n",
    "        # smap_rmse = _calculate_rmse(ds_sim['soilMoist'].isel(time=slice(100,3000)), smap_obs['soilMoist'].isel(time=slice(100,3000)))\n",
    "        # vic_rmse = _calculate_rmse(ds_sim['soilMoist'].isel(time=slice(100,3000)), vic_obs['soilMoist'].isel(time=slice(100,3000)))\n",
    "        # mosaic_rmse = _calculate_rmse(ds_sim['soilMoist'].isel(time=slice(100,3000)), mosaic_obs['soilMoist'].isel(time=slice(100,3000)))\n",
    "        # noah_rmse = _calculate_rmse(ds_sim['soilMoist'].isel(time=slice(100,3000)), noah_obs['soilMoist'].isel(time=slice(100,3000)))\n",
    "\n",
    "        smap_rmse = _calculate_rmse(ds_sim['soilMoist_clima'], smap_obs['soilMoist_clima'])\n",
    "        vic_rmse = _calculate_rmse(ds_sim['soilMoist_clima'], vic_obs['soilMoist_clima'])\n",
    "        mosaic_rmse = _calculate_rmse(ds_sim['soilMoist_clima'], mosaic_obs['soilMoist_clima'])\n",
    "        noah_rmse = _calculate_rmse(ds_sim['soilMoist_clima'], noah_obs['soilMoist_clima'])\n",
    "        \n",
    "        # smap_rmse_anom = _calculate_rmse(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), smap_obs['soilMoist_anom'].isel(time=slice(100,3000)))\n",
    "        # vic_rmse_anom = _calculate_rmse(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), vic_obs['soilMoist_anom'].isel(time=slice(100,3000)))\n",
    "        # mosaic_rmse_anom = _calculate_rmse(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), mosaic_obs['soilMoist_anom'].isel(time=slice(100,3000)))\n",
    "        # noah_rmse_anom = _calculate_rmse(ds_sim['soilMoist_anom'].isel(time=slice(100,3000)), noah_obs['soilMoist_anom'].isel(time=slice(100,3000)))\n",
    "\n",
    "        # Merge all\n",
    "        ds_out = xr.Dataset({\n",
    "            # 'smap_pearsonr': smap_corr,\n",
    "            'smap_pearsonr_anom': smap_corr_anom,\n",
    "            # 'vic_pearsonr': vic_corr,\n",
    "            'vic_pearsonr_anom': vic_corr_anom,\n",
    "            # 'noah_pearsonr': noah_corr,\n",
    "            'noah_pearsonr_anom': noah_corr_anom,\n",
    "            # 'mosaic_pearsonr': mosaic_corr,\n",
    "            'mosaic_pearsonr_anom': mosaic_corr_anom,\n",
    "            'smap_rmse_clima': smap_rmse,\n",
    "            # 'smap_rmse_anom': smap_rmse_anom,\n",
    "            'vic_rmse_clima': vic_rmse,\n",
    "            # 'vic_rmse_anom': vic_rmse_anom,\n",
    "            'noah_rmse_clima': noah_rmse,\n",
    "            # 'noah_rmse_anom': noah_rmse_anom,\n",
    "            'mosaic_rmse_clima': mosaic_rmse,\n",
    "            # 'mosaic_rmse_anom': mosaic_rmse_anom,\n",
    "        })\n",
    "        \n",
    "        # Return spatial average\n",
    "        return ds_out.mean(dim=['lat','lon']).assign_coords(iparam=[iparam]).to_dataframe()\n",
    "            \n",
    "    except Exception as e:\n",
    "         with open(f'/storage/work/dcl5300/current_projects/wbm_soilM_crop_uc_lafferty-etal-2024-tbd/code/logs/{str(iparam)}.txt', 'w') as f:\n",
    "            f.write(str(e))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27969869-158c-4919-9372-b5987caed7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delayed\n",
    "delayed = []\n",
    "\n",
    "for iparam in range(10_000):\n",
    "    tmp = dask.delayed(calculate_error_metrics)(project_data_path = project_data_path,\n",
    "                                                iparam = iparam)\n",
    "    delayed.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bebaa80-cf7a-4438-b816-49b2574b9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute\n",
    "out = dask.compute(*delayed)\n",
    "\n",
    "# Store\n",
    "pd.concat(out).to_csv(f'{project_data_path}/WBM/precalibration/centralUS/results_2step.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40da33-b55b-4ed3-bd25-ad93c683df86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
