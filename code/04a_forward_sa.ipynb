{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac40b5b-e773-4a61-872e-6260a1391095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "from functools import reduce \n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "from SALib.analyze import delta\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = \"Arial\"\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.50\n",
    "obs_colors = {'SMAP':\"#e7298a\", 'VIC':\"#d95f02\", 'NOAH':\"#7570b3\", 'MOSAIC':\"#1b9e77\"}\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "\n",
    "cm_data = np.loadtxt(\"./utils/colormaps/batlow.txt\")[::-1]\n",
    "sc_cmap = LinearSegmentedColormap.from_list(\"cmap\", cm_data, N=10)\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from utils.global_paths import project_data_path, project_code_path, loca_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3afc84ba-101a-4a5a-92ce-18cf0bbcf7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-4a73e187-7aa3-11ef-b288-00000352fe80</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.SLURMCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">SLURMCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">f4abde01</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-6d81f34d-432c-4e6f-bd9b-16cdb58cc66c</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://10.6.0.166:40449\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.6.0.166:40449' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "### Dask ###\n",
    "############\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    account=\"pches\",\n",
    "    # account=\"open\",\n",
    "    cores=1,\n",
    "    memory=\"20GiB\",\n",
    "    walltime=\"03:00:00\"\n",
    ")\n",
    "cluster.scale(jobs=30)  # ask for jobs\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491cd1f-8174-4c02-9fad-acafcb4ae040",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b58cd59-dbd8-4775-9695-b1efdc14eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info \n",
    "subset_name = 'eCONUS'\n",
    "obs_names = list(obs_colors.keys())\n",
    "\n",
    "# Metrics\n",
    "soil_metrics = ['mean-anom', '5dmin-anom', '5dmax-anom',\n",
    "                'mean-change', '5dmin-change', '5dmax-change',\n",
    "                'mean', '5dmin', '5dmax']\n",
    "\n",
    "# Soil labels for plots\n",
    "soil_labels = {\"mean-anom\": \"Annual Average \\nSoil Moisture Anomaly\", \n",
    "               \"5dmin-anom\": \"Soil Moisture Anomaly \\nDuring Driest 5 Days\",\n",
    "               \"5dmax-anom\": \"Soil Moisture Anomaly \\nDuring Wettest 5 Days\",\n",
    "               \"mean-change\": \"Annual Average \\nSoil Moisture Change\", \n",
    "               \"5dmin-change\": \"Soil Moisture Change \\nDuring Driest 5 Days\",\n",
    "               \"5dmax-change\": \"Soil Moisture Change \\nDuring Wettest 5 Days\",\n",
    "               \"mean\": \"Annual Average \\nSoil Moisture\", \n",
    "               \"5dmin\": \"Soil Moisture \\nDuring Driest 5 Days\",\n",
    "               \"5dmax\": \"Soil Moisture \\nDuring Wettest 5 Days\"}\n",
    "\n",
    "# Time slices to analyze\n",
    "time_slices = [[2030, 2039], [2050,2059], [2080,2089]]\n",
    "\n",
    "# SSPs\n",
    "ssps = ['ssp245', 'ssp370']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155d38dd-43ff-4356-9881-a20bb0f140bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# models: 27\n",
      "# model/expts: 99\n",
      "# model/expts/ens: 329\n",
      "# model/expts/ens (not including historical): 221\n"
     ]
    }
   ],
   "source": [
    "# Models \n",
    "models = os.listdir(f\"{loca_path}/\")\n",
    "models.remove('training_data')\n",
    "models.remove('scripts')\n",
    "\n",
    "loca_all = {}\n",
    "\n",
    "# Loop through models\n",
    "for model in models:\n",
    "    loca_all[model] = {}\n",
    "    # Loop through members\n",
    "    members = os.listdir(f\"{loca_path}/{model}/0p0625deg/\")\n",
    "    for member in members:\n",
    "        # Append SSPs\n",
    "        ssps = os.listdir(f\"{loca_path}/{model}/0p0625deg/{member}/\")\n",
    "        loca_all[model][member] = ssps\n",
    "\n",
    "# Matches website (https://loca.ucsd.edu/loca-version-2-for-north-america-ca-jan-2023/) as of Jan 2023\n",
    "print(f\"# models: {len(models)}\")\n",
    "print(f\"# model/expts: {np.sum([len(np.unique([item for row in [loca_all[model][member] for member in loca_all[model].keys()] for item in row])) for model in models])}\")\n",
    "print(f\"# model/expts/ens: {np.sum([len(loca_all[model][ssp]) for model in models for ssp in loca_all[model]])}\")\n",
    "print(f\"# model/expts/ens (not including historical): {np.sum([len([ssp for ssp in loca_all[model][member] if ssp != 'historical']) for model in models for member in loca_all[model]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ce2a4c-5536-4a26-b99e-e9a8a354a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all soil moisture metrics\n",
    "def read_all(subset_name, soil_metric, ssps):\n",
    "    # For all\n",
    "    ds_all = []\n",
    "\n",
    "    # Loop through models\n",
    "    for model in models:\n",
    "        # Take first member only\n",
    "        for member in list(loca_all[model].keys())[:1]:\n",
    "            # Loop through SSPs\n",
    "            for ssp in loca_all[model][member]:\n",
    "                if ssp in ssps:\n",
    "                    projection_id = f\"{model}_{member}_{ssp}\"\n",
    "                    ds_proj = []\n",
    "                    # Loop through obs\n",
    "                    for obs_name in obs_names:\n",
    "                        # Concat along loss metrics\n",
    "                        ds = xr.open_mfdataset(f\"{project_data_path}/projections/{subset_name}/metrics/{soil_metric}/{projection_id}_{obs_name}_*.nc\",\n",
    "                                                combine=\"nested\", concat_dim = \"loss_metric\")\n",
    "                        # Append\n",
    "                        ds_proj.append(ds)\n",
    "                    # Concat along obs\n",
    "                    ds_proj = xr.concat(ds_proj, dim=\"obs_name\")\n",
    "                    ds_all.append(ds_proj)\n",
    "                    \n",
    "    # Concat along climate\n",
    "    ds_all = xr.concat(ds_all, dim=\"projection_id\")\n",
    "\n",
    "    # Fix time dim\n",
    "    ds_all['time'] = ds_all['time'].dt.year\n",
    "\n",
    "    return ds_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7827e0-78d2-43a0-aba2-3624fb76856b",
   "metadata": {},
   "source": [
    "# Delta SA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ee0f0-f23e-4e68-a72a-8ec3addb0eb3",
   "metadata": {},
   "source": [
    "## Anomaly/changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57839a7a-029d-4b0f-b9ff-0986e52aaf62",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d51eb9ba-4875-47d3-807d-3eb882952887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads all .nc projections and stores as parquet file\n",
    "def store_as_df(subset_name, soil_metric, time_slice):\n",
    "    # Save path\n",
    "    save_path = f'{project_data_path}/projections/{subset_name}/metrics_df/{soil_metric}_{time_slice[0]}-{time_slice[1]}'\n",
    "    \n",
    "    # Check if done\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'{soil_metric} {time_slice} already done!')\n",
    "    else:\n",
    "        # Read all (all SSPs)\n",
    "        ssps = ['ssp245', 'ssp370']\n",
    "        ds_all = read_all(subset_name, soil_metric, ssps)\n",
    "\n",
    "        # Select \n",
    "        vars_to_drop = ['projection_id', 'member', 'val_id', 'param_id', 'soil_id']\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "            df = ds_all.sel(time=slice(time_slice[0], time_slice[1])).drop_vars(vars_to_drop).to_dask_dataframe()\n",
    "\n",
    "        # Store\n",
    "        df.to_parquet(save_path, append=False, overwrite=True, compute=True, write_index=False)\n",
    "        print(f'{soil_metric} {time_slice} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160d3771-424a-46d4-81ef-f9be358cd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read specified lat/lon and perfrom delta SA\n",
    "def get_delta(subset_name, soil_metric, time_slice, lat, lon, sa_factors):\n",
    "    # Read\n",
    "    sel = [(\"lat\", \"==\", lat), (\"lon\", \"==\", lon)]\n",
    "    file_path = f'{project_data_path}/projections/{subset_name}/metrics_df/{soil_metric}_{time_slice[0]}-{time_slice[1]}'\n",
    "    df_loc = pd.read_parquet(file_path, filters=sel, engine=\"pyarrow\")\n",
    "\n",
    "    # If needed\n",
    "    df_loc['soil_id'] = df_loc['obs_name'] + \"_\" + df_loc['loss_metric']\n",
    "\n",
    "    # Shuffle for good measure\n",
    "    df = df_loc.sample(frac=1)\n",
    "\n",
    "    # Skip if all NaN\n",
    "    if df.isnull().values.any():\n",
    "        return None\n",
    "\n",
    "    # Problem defn\n",
    "    n_factors = len(sa_factors)\n",
    "    problem = {\n",
    "        'num_vars': n_factors,\n",
    "        'names': sa_factors,\n",
    "    }\n",
    "\n",
    "    # Perform SA\n",
    "    X = df[sa_factors].to_numpy()\n",
    "    Y = df[soil_metric].to_numpy()\n",
    "    \n",
    "    Si = delta.analyze(problem, X, Y, num_resamples=2).to_df()\n",
    "    Si['lat'] = lat\n",
    "    Si['lon'] = lon\n",
    "\n",
    "    Si = Si.reset_index().pivot(index=['lat','lon'], columns='index', values='delta')\n",
    "    \n",
    "    return Si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84999b0-cf3c-4276-9997-79cc84f7cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gridpoint-level delta SA on saved parquet file\n",
    "def perform_delta_sa(subset_name, soil_metric, time_slice, sa_factors, save_name):\n",
    "    # Check if done\n",
    "    save_path = f'{project_data_path}/projections/{subset_name}/sa_results/{soil_metric}_{time_slice[0]}-{time_slice[1]}_delta-sa_{save_name}.nc'\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'{soil_metric} {time_slice} already done!')\n",
    "        return None\n",
    "        \n",
    "    # Get non-NaN locs\n",
    "    locs = np.load(f\"{project_code_path}/code/utils/grids/{subset_name}_non_nans.npy\", allow_pickle=True)\n",
    "\n",
    "    # Loop over all with dask.delayed\n",
    "    delayed = []\n",
    "\n",
    "    for loc in locs:\n",
    "        lat, lon = loc\n",
    "        df_tmp = dask.delayed(get_delta)(subset_name, soil_metric, time_slice, lat, lon, sa_factors)\n",
    "        delayed.append(df_tmp)\n",
    "    \n",
    "    # Compute\n",
    "    delayed_out = dask.compute(*delayed)\n",
    "\n",
    "    # Pandas dataframe \n",
    "    df = pd.concat(delayed_out)\n",
    "    df = df.rename_axis(None, axis=1)\n",
    "\n",
    "    # Create a complete grid of lat, lon values\n",
    "    all_lats = np.load(f\"{project_code_path}/code/utils/grids/{subset_name}_lat.npy\")\n",
    "    all_lons = np.load(f\"{project_code_path}/code/utils/grids/{subset_name}_lon.npy\")\n",
    "\n",
    "    lon, lat = np.meshgrid(all_lons, all_lats)\n",
    "    lon_lat_index = pd.MultiIndex.from_arrays([lat.flatten(), lon.flatten()], names=['lat', 'lon'])\n",
    "\n",
    "    # Reindex to include all lat, lon combinations, filling missing ones with NaN\n",
    "    df_reindexed = df.reindex(lon_lat_index)\n",
    "\n",
    "    # Convert the reindexed DataFrame to an xarray Dataset\n",
    "    ds = xr.Dataset.from_dataframe(df_reindexed)\n",
    "\n",
    "    # Store\n",
    "    ds.to_netcdf(save_path)\n",
    "    print(f'{soil_metric} {time_slice} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328cf365-724e-4a9c-b832-3e4354a827e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store non-nan coords for subset if not done already\n",
    "file_path = f\"{project_code_path}/code/utils/grids/{subset_name}_non_nans.npy\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # Read all \n",
    "    ds_all = read_all(subset_name, soil_metrics[0], ssps)\n",
    "\n",
    "    # Get non-nans\n",
    "    ds_all_stacked = ds_all.stack(loc=['lat','lon']).isel(time=10, projection_id=10, obs_name=0, loss_metric=0)[soil_metrics[0]]\n",
    "    locs = ds_all_stacked[ds_all_stacked.notnull().compute()]['loc'].to_numpy()\n",
    "\n",
    "    # Save\n",
    "    np.save(file_path, locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba44df8-6e16-438d-a07d-32900b22f580",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b557ba6-077c-4793-bb3e-eafe9466902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean-anom [2030, 2039] already done!\n",
      "mean-anom [2050, 2059] already done!\n",
      "mean-anom [2080, 2089] already done!\n",
      "5dmin-anom [2030, 2039] already done!\n",
      "5dmin-anom [2050, 2059] already done!\n",
      "5dmin-anom [2080, 2089] already done!\n",
      "5dmax-anom [2030, 2039] already done!\n",
      "5dmax-anom [2050, 2059] already done!\n",
      "5dmax-anom [2080, 2089] already done!\n",
      "mean-change [2030, 2039] already done!\n",
      "mean-change [2050, 2059] already done!\n",
      "mean-change [2080, 2089] already done!\n",
      "5dmin-change [2030, 2039] already done!\n",
      "5dmin-change [2050, 2059] already done!\n",
      "5dmin-change [2080, 2089] already done!\n",
      "5dmax-change [2030, 2039] already done!\n",
      "5dmax-change [2050, 2059] already done!\n",
      "5dmax-change [2080, 2089] already done!\n",
      "mean [2030, 2039] already done!\n",
      "mean [2050, 2059] already done!\n",
      "mean [2080, 2089] already done!\n",
      "5dmin [2030, 2039] already done!\n",
      "5dmin [2050, 2059] already done!\n",
      "5dmin [2080, 2089] already done!\n",
      "5dmax [2030, 2039] already done!\n",
      "5dmax [2050, 2059] already done!\n",
      "5dmax [2080, 2089] already done!\n"
     ]
    }
   ],
   "source": [
    "# Store as df\n",
    "for soil_metric in soil_metrics:\n",
    "    for time_slice in time_slices:\n",
    "        store_as_df(subset_name, soil_metric, time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b86a7e8-ead7-4604-b2a8-2ed8ce029a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean-anom [2050, 2059] already done!\n",
      "mean-anom [2050, 2059] already done!\n",
      "mean-anom [2080, 2089] already done!\n",
      "mean-anom [2080, 2089] already done!\n",
      "5dmin-anom [2050, 2059] already done!\n",
      "5dmin-anom [2050, 2059] already done!\n",
      "5dmin-anom [2080, 2089] already done!\n",
      "5dmin-anom [2080, 2089] already done!\n",
      "5dmax-anom [2050, 2059] already done!\n",
      "5dmax-anom [2050, 2059] already done!\n",
      "5dmax-anom [2080, 2089] already done!\n",
      "5dmax-anom [2080, 2089] already done!\n",
      "mean-change [2050, 2059] already done!\n",
      "mean-change [2050, 2059] already done!\n",
      "mean-change [2080, 2089] already done!\n",
      "mean-change [2080, 2089] already done!\n",
      "5dmin-change [2050, 2059] already done!\n",
      "5dmin-change [2050, 2059] already done!\n",
      "5dmin-change [2080, 2089] done\n",
      "5dmin-change [2080, 2089] already done!\n",
      "5dmax-change [2050, 2059] already done!\n",
      "5dmax-change [2050, 2059] already done!\n",
      "5dmax-change [2080, 2089] done\n",
      "5dmax-change [2080, 2089] already done!\n",
      "CPU times: user 19min 28s, sys: 1min 10s, total: 20min 39s\n",
      "Wall time: 1h 33min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform SA\n",
    "for soil_metric in soil_metrics[:6]: # skip absolute values\n",
    "    for time_slice in time_slices[1:]:\n",
    "        # All\n",
    "        sa_factors = ['ssp', 'model', 'time', 'obs_name', 'loss_metric']\n",
    "        save_name = 'all'\n",
    "        perform_delta_sa(subset_name, soil_metric, time_slice, sa_factors, save_name)\n",
    "    \n",
    "        # Soil grouped\n",
    "        sa_factors = ['ssp', 'model', 'time', 'soil_id']\n",
    "        save_name = 'soil_grouped'\n",
    "        perform_delta_sa(subset_name, soil_metric, time_slice, sa_factors, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bdd3c-ac38-41e6-8c99-55f39504b373",
   "metadata": {},
   "source": [
    "## Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c56a2-fcce-4a30-9ec6-66ea7483936c",
   "metadata": {},
   "source": [
    "### Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617ac812-ab6c-4cf5-bf79-9754cda4ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression function\n",
    "def linear_regression(X, y):\n",
    "    if np.isfinite(y).all() == False:\n",
    "        return np.array([np.nan, np.nan])\n",
    "    else:\n",
    "        return np.polyfit(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb40ae9-7e58-4469-9ec0-c49f8b7b141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean already done!\n",
      "5dmin already done!\n",
      "CPU times: user 1min 57s, sys: 23.8 s, total: 2min 21s\n",
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SSPs\n",
    "ssps = ['ssp245', 'ssp370']\n",
    "\n",
    "# Loop through metrics\n",
    "for soil_metric in ['mean', '5dmin', '5dmax']:\n",
    "    # Check if done\n",
    "    save_path = f'{project_data_path}/projections/eCONUS/metrics/{soil_metric}_trend.nc'\n",
    "    if not os.path.exists(save_path):\n",
    "        # Real all\n",
    "        ds = read_all(subset_name, soil_metric, ssps)\n",
    "\n",
    "        # Linear trend\n",
    "        result = xr.apply_ufunc(\n",
    "            linear_regression,\n",
    "            ds['time'],  # input x data\n",
    "            ds[soil_metric],  # input y data\n",
    "            input_core_dims=[['time'], ['time']],  # specify core dimensions for inputs\n",
    "            output_core_dims=[['coef']],  # specify core dimensions for output\n",
    "            vectorize=True,  # apply function element-wise\n",
    "            dask='parallelized',  # enable parallelization with dask\n",
    "            output_dtypes=[float],  # specify output data type\n",
    "            dask_gufunc_kwargs={\"output_sizes\": {\"coef\": 2}},\n",
    "        ).compute()\n",
    "    \n",
    "        # Store\n",
    "        ds_result = xr.Dataset({'result': result})\n",
    "        ds_result['coef'] = ['trend', 'intcp']\n",
    "        ds_result.to_netcdf(save_path)\n",
    "    else:\n",
    "        print(f\"{soil_metric} already done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba19f89-965b-4833-9524-a6d2c4313276",
   "metadata": {},
   "source": [
    "### SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fbd2875-14b1-473a-be0b-74ec4448b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read specified lat/lon and perfrom delta SA\n",
    "def get_delta(ds, lat, lon, sa_factors):\n",
    "    # Select\n",
    "    df_loc = ds.sel(coef='trend').sel(lat=lat, lon=lon).to_dataframe().reset_index()\n",
    "\n",
    "    # If needed\n",
    "    df_loc['soil_id'] = df_loc['obs_name'] + \"_\" + df_loc['loss_metric']\n",
    "\n",
    "    # Shuffle for good measure\n",
    "    df = df_loc.sample(frac=1)\n",
    "\n",
    "    # Skip if all NaN\n",
    "    if df.isnull().values.any():\n",
    "        return None\n",
    "\n",
    "    # Problem defn\n",
    "    n_factors = len(sa_factors)\n",
    "    problem = {\n",
    "        'num_vars': n_factors,\n",
    "        'names': sa_factors,\n",
    "    }\n",
    "\n",
    "    # Perform SA\n",
    "    X = df[sa_factors].to_numpy()\n",
    "    Y = df['result'].to_numpy()\n",
    "    \n",
    "    Si = delta.analyze(problem, X, Y, num_resamples=2).to_df()\n",
    "    Si['lat'] = lat\n",
    "    Si['lon'] = lon\n",
    "\n",
    "    Si = Si.reset_index().pivot(index=['lat','lon'], columns='index', values='delta')\n",
    "    \n",
    "    return Si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68338952-8747-4fde-9c54-f55b410eeb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 50s, sys: 16.4 s, total: 7min 7s\n",
      "Wall time: 25min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Choose SA grouping\n",
    "# sa_factors = ['soil_id', 'ssp', 'model']\n",
    "# save_name = 'soil_grouped'\n",
    "\n",
    "sa_factors = ['ssp', 'model', 'obs_name', 'loss_metric']\n",
    "save_name = 'all'\n",
    "\n",
    "# Loop through metrics\n",
    "for soil_metric in ['mean', '5dmin', '5dmax']:\n",
    "    save_path = f'{project_data_path}/projections/{subset_name}/sa_results/{soil_metric}_trends_delta-sa_{save_name}.nc'\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'{soil_metric} already done!')\n",
    "\n",
    "    else:\n",
    "        # Read trends\n",
    "        ds = xr.open_dataset(f'{project_data_path}/projections/{subset_name}/metrics/{soil_metric}_trend.nc')\n",
    "    \n",
    "        # Get non-NaN locs\n",
    "        locs = np.load(f\"{project_code_path}/code/utils/grids/{subset_name}_non_nans.npy\", allow_pickle=True)\n",
    "    \n",
    "        # Loop over all with dask.delayed\n",
    "        delayed = []\n",
    "        for loc in locs:\n",
    "            lat, lon = loc\n",
    "            df_tmp = dask.delayed(get_delta)(ds, lat, lon, sa_factors)\n",
    "            delayed.append(df_tmp)\n",
    "    \n",
    "        # Compute\n",
    "        delayed_out = dask.compute(*delayed)\n",
    "    \n",
    "        # Pandas dataframe \n",
    "        df = pd.concat(delayed_out)\n",
    "        df = df.rename_axis(None, axis=1)\n",
    "        \n",
    "        # Create a complete grid of lat, lon values\n",
    "        all_lats = np.load(f\"{project_code_path}/code/utils/grids/{subset_name}_lat.npy\")\n",
    "        all_lons = np.load(f\"{project_code_path}/code/utils/grids/{subset_name}_lon.npy\")\n",
    "        \n",
    "        lon, lat = np.meshgrid(all_lons, all_lats)\n",
    "        lon_lat_index = pd.MultiIndex.from_arrays([lat.flatten(), lon.flatten()], names=['lat', 'lon'])\n",
    "        \n",
    "        # Reindex to include all lat, lon combinations, filling missing ones with NaN\n",
    "        df_reindexed = df.reindex(lon_lat_index)\n",
    "        \n",
    "        # Convert the reindexed DataFrame to an xarray Dataset\n",
    "        ds_out = xr.Dataset.from_dataframe(df_reindexed)\n",
    "    \n",
    "        # Store\n",
    "        ds_out.to_netcdf(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28f91b-277d-475c-97dc-f19a1e2451ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
